# Week 3: Real-time Detection with YOLOv8

## ğŸ“š Má»¥c tiÃªu há»c táº­p

Tuáº§n nÃ y báº¡n sáº½ há»c:
1. **Object Detection** - PhÃ¡t hiá»‡n vÃ  localize objects
2. **YOLOv8 Architecture** - State-of-the-art detector
3. **Integration** - Káº¿t há»£p detection + classification
4. **Real-time Processing** - Webcam inference

---

## ğŸ¯ LÃ½ thuyáº¿t Object Detection

### 1. Object Detection vs Classification

**Image Classification:**
```
Input: áº¢nh â†’ Output: Class label
"This is a plastic bottle"
```

**Object Detection:**
```
Input: áº¢nh â†’ Output: [Bounding boxes + Class labels]
"There are 2 objects:
  - plastic bottle at (x1, y1, x2, y2)
  - paper cup at (x3, y3, x4, y4)"
```

**Sá»± khÃ¡c biá»‡t:**
- ğŸ¯ Detection: TÃ¬m **WHERE** vÃ  **WHAT**
- ğŸ“ Localization: Bounding box coordinates
- ğŸ”¢ Multiple objects trong 1 áº£nh

### 2. YOLO (You Only Look Once)

**Ã tÆ°á»Ÿng chÃ­nh:**
- Single-stage detector (chá»‰ 1 láº§n forward pass)
- Nhanh hÆ¡n two-stage (R-CNN, Faster R-CNN)
- Real-time: 30-60 FPS

**Evolution:**
```
YOLOv1 (2016) â†’ YOLOv3 (2018) â†’ YOLOv5 (2020) 
â†’ YOLOv8 (2023) â† We use this!
```

#### YOLOv8 Architecture

**Backbone:** CSPDarknet (feature extraction)
```
Input (640Ã—640)
  â†“
Conv layers + CSP blocks
  â†“
Feature maps at 3 scales:
  - P3: 80Ã—80 (small objects)
  - P4: 40Ã—40 (medium objects)
  - P5: 20Ã—20 (large objects)
```

**Neck:** PAN (Path Aggregation Network)
- Káº¿t há»£p multi-scale features
- Bottom-up + Top-down paths

**Head:** Detection head
- PhÃ¡t hiá»‡n objects á»Ÿ má»—i scale
- Output: [x, y, w, h, confidence, class_probs]

### 3. Key Concepts

#### a) Bounding Box Format

**YOLO format: (x_center, y_center, width, height)**
```python
# Normalized [0, 1]
bbox = [0.5, 0.5, 0.3, 0.4]
# â†’ Object á»Ÿ giá»¯a áº£nh, width=30%, height=40%
```

**Chuyá»ƒn sang (x1, y1, x2, y2):**
```python
x1 = x_center - width/2
y1 = y_center - height/2
x2 = x_center + width/2
y2 = y_center + height/2
```

#### b) Confidence Score

**CÃ´ng thá»©c:**
```
Confidence = P(object) Ã— IoU(pred, truth)
```

- `P(object)`: XÃ¡c suáº¥t cÃ³ object trong box
- `IoU`: Intersection over Union vá»›i ground truth

**Threshold:** 
- Confidence > 0.5 â†’ Keep detection
- Confidence < 0.5 â†’ Discard

#### c) IoU (Intersection over Union)

**CÃ´ng thá»©c:**
```
IoU = Area(Intersection) / Area(Union)
```

**Ã nghÄ©a:**
- IoU = 1.0: Perfect overlap
- IoU > 0.5: Good detection
- IoU < 0.3: Poor detection

**Sá»­ dá»¥ng:**
- ÄÃ¡nh giÃ¡ quality cá»§a bounding box
- Non-Maximum Suppression (NMS)

#### d) Non-Maximum Suppression (NMS)

**Váº¥n Ä‘á»:** Nhiá»u boxes detect cÃ¹ng 1 object

**Giáº£i phÃ¡p NMS:**
```
1. Sort boxes by confidence (high â†’ low)
2. Keep box vá»›i confidence cao nháº¥t
3. Remove boxes cÃ³ IoU > threshold vá»›i box Ä‘Ã£ chá»n
4. Repeat cho boxes cÃ²n láº¡i
```

**Parameters:**
- `iou_threshold`: 0.45 (default)
- Cao â†’ giá»¯ nhiá»u boxes (nhiá»…u)
- Tháº¥p â†’ giá»¯ Ã­t boxes (miss objects)

---

## ğŸ”— Pipeline Integration

### Detection + Classification

**Flow:**
```
[Input: Camera frame]
        â†“
[YOLOv8 Detection]
    â†’ Detects generic "objects"
    â†’ Output: Bounding boxes
        â†“
[Crop detected regions]
        â†“
[MobileNetV2 Classification]
    â†’ Classifies waste type
    â†’ Output: Class labels
        â†“
[Draw results on frame]
    â†’ Bounding box + Label
        â†“
[Display: Real-time video]
```

**Táº¡i sao dÃ¹ng 2 models?**

1. **YOLOv8:** KhÃ´ng train láº¡i, dÃ¹ng pretrained
   - PhÃ¡t hiá»‡n "objects" tá»•ng quÃ¡t
   - Nhanh, real-time
   
2. **MobileNetV2:** Custom classifier Ä‘Ã£ train
   - PhÃ¢n loáº¡i waste types (10 classes)
   - Accuracy cao (~90%)

**Alternative approach:**
- Train YOLOv8 trá»±c tiáº¿p trÃªn waste dataset
- â†’ Cáº§n annotate bounding boxes (tá»‘n thá»i gian)
- â†’ Our approach nhanh hÆ¡n, leverage pretrained models

---

## ğŸ“‚ Cáº¥u trÃºc

```
Week3_Realtime_Detection/
â”œâ”€â”€ README.md                    # File nÃ y
â”œâ”€â”€ assignments/                 # Notebook
â”‚   â””â”€â”€ W3_Integration.ipynb
â”œâ”€â”€ realtime_detection.py        # Main script
â””â”€â”€ utils/
    â”œâ”€â”€ detection_utils.py       # YOLOv8 wrapper
    â””â”€â”€ realtime_utils.py        # Classification + drawing
```

---

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### Python Script

```bash
# DÃ¹ng webcam máº·c Ä‘á»‹nh
python Week3_Realtime_Detection/realtime_detection.py

# DÃ¹ng camera khÃ¡c
python Week3_Realtime_Detection/realtime_detection.py --camera 1

# DÃ¹ng video file
python Week3_Realtime_Detection/realtime_detection.py --video path/to/video.mp4

# Chá»n model classifier
python Week3_Realtime_Detection/realtime_detection.py --model mobilenetv2

# Hoáº·c main.py
python main.py --realtime
```

### Controls khi cháº¡y

```
ğŸ“¹ Real-time Detection Window
â”œâ”€â”€ 'q': Quit/Exit
â”œâ”€â”€ 's': Save screenshot
â””â”€â”€ 'p': Pause/Resume
```

---

## âš¡ Performance Optimization

### 1. Frame Skipping

**Váº¥n Ä‘á»:** Processing má»i frame â†’ slow

**Giáº£i phÃ¡p:**
```python
if frame_count % 2 == 0:  # Process every 2nd frame
    detections = detect(frame)
```

**Trade-off:**
- âœ… 2x faster
- âš ï¸ Less responsive

### 2. Model Optimization

**YOLOv8 variants:**
```
YOLOv8n: Nano    - 3.2M params - 80 FPS â† We use
YOLOv8s: Small   - 11M params  - 60 FPS
YOLOv8m: Medium  - 25M params  - 45 FPS
YOLOv8l: Large   - 43M params  - 30 FPS
```

**Recommendation:** YOLOv8n cho real-time

### 3. Resolution

**Input size:**
```
640Ã—640: Best accuracy, slower
320Ã—320: Lower accuracy, 4x faster
```

**Webcam resolution:**
```python
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
```

### 4. Batching (GPU)

**Náº¿u cÃ³ GPU:**
```python
# Process multiple crops cÃ¹ng lÃºc
crops = [crop1, crop2, crop3, ...]
predictions = model.predict(batch(crops))
```

---

## ğŸ“Š Specs

### Hardware Requirements

**Minimum:**
- CPU: Intel i5 hoáº·c equivalent
- RAM: 8GB
- Webcam: 720p

**Recommended:**
- GPU: NVIDIA GTX 1060 hoáº·c cao hÆ¡n
- RAM: 16GB
- Webcam: 1080p

### Performance Metrics

| Setup | FPS | Latency |
|-------|-----|---------|
| CPU only (i7) | 10-15 | ~100ms |
| GPU (GTX 1060) | 25-30 | ~40ms |
| GPU (RTX 3060) | 40-50 | ~25ms |

---

## ğŸ”§ Troubleshooting

### Camera khÃ´ng má»Ÿ Ä‘Æ°á»£c

```python
# Thá»­ cÃ¡c camera index khÃ¡c
python realtime_detection.py --camera 0  # Default
python realtime_detection.py --camera 1  # External
```

### FPS tháº¥p

**Solutions:**
1. Giáº£m resolution: `--resolution 320`
2. DÃ¹ng YOLOv8n (nano)
3. Skip frames: process every 2-3 frames
4. ÄÃ³ng cÃ¡c apps khÃ¡c

### Detection khÃ´ng chÃ­nh xÃ¡c

**Adjustments:**
```python
# TÄƒng confidence threshold
YOLO_CONFIDENCE = 0.7  # default: 0.5

# Äiá»u chá»‰nh NMS threshold
YOLO_IOU_THRESHOLD = 0.3  # default: 0.45
```

---

## ğŸ“ Kiáº¿n thá»©c há»c Ä‘Æ°á»£c

Sau Week 3, báº¡n sáº½ hiá»ƒu:

âœ… Object detection vs classification  
âœ… YOLO architecture vÃ  cÃ¡ch hoáº¡t Ä‘á»™ng  
âœ… IoU, NMS, confidence score  
âœ… Káº¿t há»£p detection + classification  
âœ… Real-time processing techniques  
âœ… Optimization strategies cho FPS cao  

---

## ğŸ”— TÃ i liá»‡u tham kháº£o

1. **YOLO:**
   - "YOLOv8: State-of-the-art Object Detection" - Ultralytics
   - YOLOv1 paper: "You Only Look Once" (2016)

2. **Object Detection:**
   - CS231n: Detection and Segmentation
   - "Object Detection in 20 Years: A Survey" (2019)

3. **Real-time Processing:**
   - OpenCV Documentation
   - "Real-Time Object Detection with YOLO"

---

**Previous:** [Week 2 - Transfer Learning](../Week2_Transfer_Learning/README.md)  
**Next:** [Week 4 - Model Optimization](../Week4_Deployment/README.md)

