# Week 1: Data Preparation and Baseline CNN

## ğŸ“š Má»¥c tiÃªu há»c táº­p

Tuáº§n nÃ y báº¡n sáº½ há»c:
1. **KhÃ¡m phÃ¡ dá»¯ liá»‡u (Data Exploration)** - PhÃ¢n tÃ­ch vÃ  visualize dataset
2. **Tiá»n xá»­ lÃ½ dá»¯ liá»‡u (Data Preprocessing)** - Chia train/val/test, augmentation
3. **Baseline CNN** - XÃ¢y dá»±ng máº¡ng CNN tá»« Ä‘áº§u

---

## ğŸ¯ LÃ½ thuyáº¿t cÆ¡ báº£n

### 1. Convolutional Neural Networks (CNN)

CNN lÃ  kiáº¿n trÃºc deep learning Ä‘áº·c biá»‡t hiá»‡u quáº£ cho xá»­ lÃ½ áº£nh.

**CÃ¡c thÃ nh pháº§n chÃ­nh:**

#### a) Convolutional Layer
- **CÃ´ng thá»©c:** `Output = (Input * Kernel) + Bias`
- **Má»¥c Ä‘Ã­ch:** TrÃ­ch xuáº¥t features tá»« áº£nh (edges, textures, patterns)
- **Parameters:** Filters (sá»‘ lÆ°á»£ng kernels), kernel size (thÆ°á»ng 3Ã—3 hoáº·c 5Ã—5)

#### b) Activation Function (ReLU)
- **CÃ´ng thá»©c:** `f(x) = max(0, x)`
- **Má»¥c Ä‘Ã­ch:** ThÃªm tÃ­nh phi tuyáº¿n vÃ o mÃ´ hÃ¬nh
- **Lá»£i Ã­ch:** Nhanh, trÃ¡nh vanishing gradient

#### c) Pooling Layer
- **Max Pooling:** Láº¥y giÃ¡ trá»‹ lá»›n nháº¥t trong vÃ¹ng
- **Average Pooling:** Láº¥y trung bÃ¬nh
- **Má»¥c Ä‘Ã­ch:** Giáº£m kÃ­ch thÆ°á»›c, tÄƒng tÃ­nh báº¥t biáº¿n vá»‹ trÃ­

#### d) Batch Normalization
- **CÃ´ng thá»©c:** `y = Î³ * (x - Î¼) / Ïƒ + Î²`
- **Má»¥c Ä‘Ã­ch:** á»”n Ä‘á»‹nh quÃ¡ trÃ¬nh training, tÄƒng tá»‘c Ä‘á»™ há»c

#### e) Dropout
- **CÆ¡ cháº¿:** Randomly "táº¯t" má»™t pháº§n neurons trong training
- **Má»¥c Ä‘Ã­ch:** Regularization, trÃ¡nh overfitting
- **Rate:** ThÆ°á»ng dÃ¹ng 0.5 (táº¯t 50% neurons)

### 2. Data Augmentation

TÄƒng cÆ°á»ng dá»¯ liá»‡u Ä‘á»ƒ model há»c tá»‘t hÆ¡n vÃ  trÃ¡nh overfitting.

**CÃ¡c ká»¹ thuáº­t:**
- **Horizontal Flip:** Láº­t áº£nh ngang
- **Rotation:** Xoay áº£nh Â±10-20Â°
- **Zoom:** PhÃ³ng to/thu nhá»
- **Contrast:** Äiá»u chá»‰nh Ä‘á»™ tÆ°Æ¡ng pháº£n

**LÆ°u Ã½:** Chá»‰ Ã¡p dá»¥ng cho training set, KHÃ”NG cho val/test set.

### 3. Training Process

**Loss Function:** Categorical Cross-Entropy
```
L = -Î£ y_true * log(y_pred)
```

**Optimizer:** Adam (Adaptive Moment Estimation)
- Káº¿t há»£p momentum vÃ  RMSprop
- Learning rate tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh
- Default: lr=0.001, Î²â‚=0.9, Î²â‚‚=0.999

**Callbacks:**
- **EarlyStopping:** Dá»«ng khi val_loss khÃ´ng cáº£i thiá»‡n
- **ReduceLROnPlateau:** Giáº£m learning rate khi stuck
- **ModelCheckpoint:** LÆ°u best model

---

## ğŸ“‚ Cáº¥u trÃºc

```
Week1_Data_and_Baseline/
â”œâ”€â”€ README.md                    # File nÃ y - lÃ½ thuyáº¿t
â”œâ”€â”€ assignments/                 # Notebooks (há»c táº­p, tÆ°Æ¡ng tÃ¡c)
â”‚   â”œâ”€â”€ W1_Data_Exploration.ipynb
â”‚   â”œâ”€â”€ W1_Preprocessing.ipynb
â”‚   â””â”€â”€ W1_Baseline_CNN.ipynb
â”œâ”€â”€ data_exploration.py          # Script (production, cháº¡y nhanh)
â”œâ”€â”€ preprocessing.py
â”œâ”€â”€ baseline_training.py
â””â”€â”€ utils/                       # Helper functions
    â”œâ”€â”€ data_utils.py
    â””â”€â”€ model_utils.py
```

---

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### Option 1: Python Scripts (Nhanh, Production)

```bash
# 1. KhÃ¡m phÃ¡ dá»¯ liá»‡u
python Week1_Data_and_Baseline/data_exploration.py

# 2. Tiá»n xá»­ lÃ½
python Week1_Data_and_Baseline/preprocessing.py

# 3. Train baseline model
python Week1_Data_and_Baseline/baseline_training.py

# Hoáº·c dÃ¹ng main.py
python main.py --week 1
```

### Option 2: Jupyter Notebooks (Há»c táº­p, TÆ°Æ¡ng tÃ¡c)

```bash
cd Week1_Data_and_Baseline/assignments
jupyter notebook W1_Data_Exploration.ipynb
```

**Khi nÃ o dÃ¹ng gÃ¬?**
- ğŸ““ **Notebooks:** Khi báº¡n muá»‘n há»c, thá»­ nghiá»‡m, xem output tá»«ng bÆ°á»›c
- ğŸ **Scripts:** Khi báº¡n muá»‘n cháº¡y full pipeline nhanh chÃ³ng

---

## ğŸ“Š Kiáº¿n trÃºc Baseline CNN

```
Input (224Ã—224Ã—3)
    â†“
[Conv Block 1: 32 filters]
    Conv2D(32, 3Ã—3, ReLU) â†’ Conv2D(32, 3Ã—3, ReLU)
    â†’ BatchNorm â†’ MaxPool(2Ã—2)
    â†“
[Conv Block 2: 64 filters]
    Conv2D(64, 3Ã—3, ReLU) â†’ Conv2D(64, 3Ã—3, ReLU)
    â†’ BatchNorm â†’ MaxPool(2Ã—2)
    â†“
[Conv Block 3: 128 filters]
    Conv2D(128, 3Ã—3, ReLU) â†’ Conv2D(128, 3Ã—3, ReLU)
    â†’ BatchNorm â†’ MaxPool(2Ã—2)
    â†“
[Conv Block 4: 256 filters]
    Conv2D(256, 3Ã—3, ReLU) â†’ Conv2D(256, 3Ã—3, ReLU)
    â†’ BatchNorm â†’ MaxPool(2Ã—2)
    â†“
GlobalAveragePooling2D
    â†“
Dense(128, ReLU) â†’ Dropout(0.5)
    â†“
Dense(10, Softmax)
```

**Sá»‘ parameters:** ~10M  
**Expected accuracy:** 75-80%

---

## ğŸ“ˆ Káº¿t quáº£ mong Ä‘á»£i

Sau khi hoÃ n thÃ nh Week 1:

âœ… Hiá»ƒu Ä‘Æ°á»£c cáº¥u trÃºc dataset (10 classes, ~20k images)  
âœ… Biáº¿t cÃ¡ch split data vÃ  augmentation  
âœ… XÃ¢y dá»±ng Ä‘Æ°á»£c CNN tá»« Ä‘áº§u  
âœ… Äáº¡t ~75-80% accuracy trÃªn test set  
âœ… Hiá»ƒu overfitting vÃ  cÃ¡ch kháº¯c phá»¥c  

---

## ğŸ”— TÃ i liá»‡u tham kháº£o

1. **CNN Architecture:**
   - CS231n: Convolutional Neural Networks - Stanford
   - Deep Learning Specialization - Andrew Ng (Coursera)

2. **Data Augmentation:**
   - "The Effectiveness of Data Augmentation in Image Classification" (2017)
   - Keras Documentation: ImageDataGenerator

3. **Training Techniques:**
   - "Batch Normalization: Accelerating Deep Network Training" (2015)
   - "Dropout: A Simple Way to Prevent Overfitting" (2014)

---

## ğŸ’¡ Tips há»c táº­p

1. **Cháº¡y notebooks trÆ°á»›c** Ä‘á»ƒ hiá»ƒu tá»«ng bÆ°á»›c
2. **Xem visualization** cá»§a filters vÃ  feature maps
3. **Thá»­ nghiá»‡m** vá»›i cÃ¡c hyperparameters khÃ¡c nhau
4. **So sÃ¡nh** model cÃ³/khÃ´ng cÃ³ augmentation
5. **Ghi chÃº** nhá»¯ng observations quan trá»ng

---

**Next:** [Week 2 - Transfer Learning](../Week2_Transfer_Learning/README.md)

