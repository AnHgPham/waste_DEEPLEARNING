{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Model Optimization for Deployment\n",
    "\n",
    "**Objective:** To prepare our trained model for efficient deployment on various platforms, including edge devices.\n",
    "\n",
    "This notebook covers:\n",
    "- Converting the Keras model to the TensorFlow Lite (TFLite) format.\n",
    "- Applying post-training INT8 quantization to reduce model size and latency.\n",
    "- Evaluating the performance trade-offs of the optimized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\", \"..\")))\n",
    "from config import *\n",
    "from Week1_Data_and_Baseline.utils.data_utils import create_data_generators\n",
    "from Week4_Deployment.utils.optimization_utils import convert_to_tflite, quantize_model, evaluate_tflite_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Load the Trained Model and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final trained Keras model\n",
    "model = keras.models.load_model(get_model_path(\"mobilenetv2\", \"final\"))\n",
    "\n",
    "# Create a data generator for the test set\n",
    "_, test_ds = create_data_generators(TRAIN_DIR, TEST_DIR, IMG_SIZE, BATCH_SIZE, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Convert to TensorFlow Lite (FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_fp32_path = get_model_path(\"mobilenetv2_fp32\", \"tflite\")\n",
    "convert_to_tflite(model, tflite_fp32_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Apply Post-Training Quantization (INT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_int8_path = get_model_path(\"mobilenetv2_int8\", \"tflite\")\n",
    "quantize_model(model, tflite_int8_path, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Compare Model Sizes and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_size = os.path.getsize(get_model_path(\"mobilenetv2\", \"final\")) / (1024 * 1024)\n",
    "tflite_fp32_size = os.path.getsize(tflite_fp32_path) / (1024 * 1024)\n",
    "tflite_int8_size = os.path.getsize(tflite_int8_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"Keras Model size: {keras_model_size:.2f} MB\")\n",
    "print(f\"TFLite FP32 Model size: {tflite_fp32_size:.2f} MB\")\n",
    "print(f\"TFLite INT8 Model size: {tflite_int8_size:.2f} MB\")\n",
    "\n",
    "print('\nEvaluating TFLite FP32 model...')\n",
    "fp32_accuracy = evaluate_tflite_model(tflite_fp32_path, test_ds)\n",
    "print(f\"TFLite FP32 Accuracy: {fp32_accuracy:.2%}\")\n",
    "\n",
    "print('\nEvaluating TFLite INT8 model...')\n",
    "int8_accuracy = evaluate_tflite_model(tflite_int8_path, test_ds)\n",
    "print(f\"TFLite INT8 Accuracy: {int8_accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
