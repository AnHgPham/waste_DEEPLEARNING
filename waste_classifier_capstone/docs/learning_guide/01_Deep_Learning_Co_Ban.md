# ğŸ§  DEEP LEARNING CÆ  Báº¢N

**Thá»i gian:** 1 giá»
**Má»¥c tiÃªu:** Hiá»ƒu Neural Network hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o

---

## ğŸ“Œ 1. NEURAL NETWORK LÃ€ GÃŒ? (10 phÃºt)

### **Äá»‹nh nghÄ©a Ä‘Æ¡n giáº£n:**

**Neural Network (Máº¡ng tháº§n kinh) = Thuáº­t toÃ¡n há»c tá»« data**

```
Input (áº£nh rÃ¡c)  â†’  Neural Network  â†’  Output (loáº¡i rÃ¡c)
[224x224x3]      â†’  [magic box]     â†’  [plastic/glass/metal...]
```

### **VÃ­ dá»¥ dá»… hiá»ƒu:**

Giá»‘ng nhÆ° **nÃ£o ngÆ°á»i há»c nháº­n dáº¡ng:**

```
Em bÃ© nhÃ¬n mÃ¨o:
Láº§n 1: "ÄÃ¢y lÃ  gÃ¬?" â†’ Máº¹: "MÃ¨o!"
Láº§n 2: "CÃ³ tai nhá»n, rÃ¢u" â†’ "MÃ¨o!"
Láº§n 3: "KÃªu meo meo" â†’ "MÃ¨o!"
...
Sau 100 láº§n â†’ Em bÃ© Tá»° Äá»˜NG nháº­n ra mÃ¨o!

Neural Network:
Láº§n 1: NhÃ¬n plastic bottle â†’ Label: "plastic"
Láº§n 2: "Trong suá»‘t, hÃ¬nh trá»¥" â†’ "plastic"
Láº§n 3: "CÃ³ náº¯p váº·n" â†’ "plastic"
...
Sau 15,777 áº£nh â†’ Model Tá»° Äá»˜NG phÃ¢n loáº¡i!
```

---

## ğŸ“Š 2. Cáº¤U TRÃšC NEURAL NETWORK (15 phÃºt)

### **A. Neuron (NÆ¡-ron)**

**Neuron = 1 Ä‘Æ¡n vá»‹ tÃ­nh toÃ¡n**

```python
# CÃ´ng thá»©c 1 neuron:
output = activation(weight * input + bias)

# VÃ­ dá»¥:
input = [0.5, 0.8, 0.3]  # 3 features
weight = [0.2, 0.5, -0.3] # Há»c Ä‘Æ°á»£c
bias = 0.1                # Há»c Ä‘Æ°á»£c

z = (0.5*0.2) + (0.8*0.5) + (0.3*-0.3) + 0.1
  = 0.1 + 0.4 - 0.09 + 0.1
  = 0.51

output = ReLU(0.51) = 0.51 (náº¿u > 0)
```

**Visualize:**
```
Input 1 (0.5) â”€â”€[w=0.2]â”€â”€â”
                          â”œâ”€â†’ Î£ + bias â”€â”€â†’ ReLU â”€â”€â†’ Output (0.51)
Input 2 (0.8) â”€â”€[w=0.5]â”€â”€â”¤
                          â”‚
Input 3 (0.3) â”€â”€[w=-0.3]â”€â”˜
```

---

### **B. Layer (Lá»›p)**

**Layer = Nhiá»u neurons cÃ¹ng nhau**

```
Input Layer:     3 neurons (input features)
Hidden Layer 1:  10 neurons
Hidden Layer 2:  10 neurons
Output Layer:    10 neurons (10 waste classes)
```

**Visualize:**
```
Input (3)    Hidden 1 (10)    Hidden 2 (10)    Output (10)
   â—               â—                â—              â— plastic
   â—â”€â”€â”         â—  â—              â—  â—            â— glass
   â—  â”œâ”€â”€â”€â”€â”€â†’  â—  â—  â—  â”€â”€â”€â”€â”€â†’  â—  â—  â—  â”€â”€â”€â”€â”€â†’  â— metal
      â””â”€â”€â”€â†’    â—  â—  â—          â—  â—  â—          â— paper
               â—  â—              â—  â—            â— ...
               â—                 â—
```

---

### **C. Activation Functions (HÃ m kÃ­ch hoáº¡t)**

**Táº¡i sao cáº§n?** â†’ Äá»ƒ model há»c Ä‘Æ°á»£c non-linear patterns!

#### **1. ReLU (Rectified Linear Unit)** â­ PHá»” BIáº¾N NHáº¤T

```python
def ReLU(x):
    return max(0, x)

# VÃ­ dá»¥:
ReLU(5) = 5
ReLU(-3) = 0
ReLU(0) = 0
```

**Graph:**
```
y
â”‚     â•±
â”‚    â•±
â”‚   â•±
â”‚  â•±
â”‚ â•±
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
  0
```

**DÃ¹ng khi:** Hidden layers (háº§u háº¿t trÆ°á»ng há»£p)

---

#### **2. Softmax** â­ OUTPUT LAYER

```python
def softmax(x):
    exp_x = np.exp(x)
    return exp_x / np.sum(exp_x)

# VÃ­ dá»¥:
logits = [2.0, 1.0, 0.1]  # raw outputs
softmax([2.0, 1.0, 0.1]) = [0.659, 0.242, 0.099]
                            â†‘
                        Tá»•ng = 1.0 (100%)
```

**DÃ¹ng khi:** Multi-class classification (plastic/glass/metal...)

---

## ğŸ”„ 3. FORWARD PROPAGATION (15 phÃºt)

**Forward Prop = TÃ­nh output tá»« input**

### **VÃ­ dá»¥ cá»¥ thá»ƒ:**

```python
# Giáº£ sá»­ classify áº£nh 28x28 (mnist digit)
input = [pixel_1, pixel_2, ..., pixel_784]  # 784 pixels

# Layer 1: 784 â†’ 128 neurons
hidden1 = ReLU(W1 @ input + b1)  # [128,]

# Layer 2: 128 â†’ 64 neurons
hidden2 = ReLU(W2 @ hidden1 + b2)  # [64,]

# Output: 64 â†’ 10 classes (0-9 digits)
output = Softmax(W3 @ hidden2 + b3)  # [10,]

# Káº¿t quáº£:
output = [0.01, 0.02, 0.05, 0.80, 0.03, ...]
                            â†‘
                    Class 3 cÃ³ prob cao nháº¥t
                    â†’ Predict: "3"
```

### **Trong dá»± Ã¡n Waste Classification:**

```python
# Input: áº¢nh waste 224x224x3
x = load_image("plastic_bottle.jpg")  # [224, 224, 3]

# Forward qua Baseline CNN:
x = Rescaling(x)                      # [0, 255] â†’ [0, 1]
x = Conv2D_32(x)                      # â†’ [112, 112, 32]
x = MaxPool(x)                        # â†’ [56, 56, 32]
x = Conv2D_64(x)                      # â†’ [56, 56, 64]
... (nhiá»u layers)
x = Dense_128(x)                      # â†’ [128,]
output = Dense_10(x)                  # â†’ [10,]

# Softmax:
probabilities = [
    0.02,  # battery
    0.01,  # biological
    0.05,  # cardboard
    0.02,  # clothes
    0.03,  # glass
    0.01,  # metal
    0.02,  # paper
    0.82,  # plastic â† HIGHEST!
    0.01,  # shoes
    0.01   # trash
]

Prediction: "plastic" âœ…
```

---

## ğŸ”™ 4. BACKPROPAGATION (15 phÃºt)

**Backprop = Há»c tá»« sai láº§m**

### **Quy trÃ¬nh:**

```
1. Forward Prop â†’ Predict
2. TÃ­nh Loss (sai bao nhiÃªu?)
3. Backward Prop â†’ TÃ­nh gradient
4. Update weights â†’ Model há»c!
```

### **VÃ­ dá»¥ Ä‘Æ¡n giáº£n:**

```python
# Ground truth: "plastic"
true_label = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
                                   â†‘ plastic = 1

# Prediction:
prediction = [0.02, 0.01, 0.05, 0.02, 0.03, 0.01, 0.02, 0.82, 0.01, 0.01]

# Loss (ì–¼ë§ˆë‚˜ sai):
loss = -log(0.82) = 0.198  # CÃ ng gáº§n 1 cÃ ng tá»‘t!

# Backprop:
# "plastic probability quÃ¡ tháº¥p (0.82), cáº§n tÄƒng lÃªn!"
# â†’ Adjust weights Ä‘á»ƒ láº§n sau predict plastic = 0.95

# Update:
weights_new = weights_old - learning_rate * gradient
```

### **Trong thá»±c táº¿:**

```
Epoch 1:
  Image 1 (plastic) â†’ Predict plastic (0.6) â†’ Loss = 0.51
  â†’ Backprop â†’ Update weights

  Image 2 (glass) â†’ Predict glass (0.7) â†’ Loss = 0.36
  â†’ Backprop â†’ Update weights

  ... (15,777 images)

Epoch 2:
  Image 1 (plastic) â†’ Predict plastic (0.75) â†‘ Better!
  ... Model Ä‘ang há»c!

Epoch 30:
  Image 1 (plastic) â†’ Predict plastic (0.95) âœ… Very good!
```

---

## ğŸ“‰ 5. LOSS FUNCTION (10 phÃºt)

**Loss = Äá»™ Ä‘o sai láº§m**

### **A. Categorical Cross-Entropy** â­ DÃ¹ng trong dá»± Ã¡n

```python
loss = -Î£ (y_true * log(y_pred))

# VÃ­ dá»¥:
y_true = [0, 0, 1, 0]  # Class 2 (plastic)
y_pred = [0.1, 0.2, 0.6, 0.1]

loss = -(0*log(0.1) + 0*log(0.2) + 1*log(0.6) + 0*log(0.1))
     = -log(0.6)
     = 0.51

# Náº¿u predict tá»‘t hÆ¡n:
y_pred = [0.05, 0.05, 0.85, 0.05]
loss = -log(0.85) = 0.16  â† Tháº¥p hÆ¡n = Tá»‘t hÆ¡n!
```

**Má»¥c tiÃªu training:** Minimize loss!

---

## ğŸ¯ 6. OPTIMIZER (10 phÃºt)

**Optimizer = Thuáº­t toÃ¡n update weights**

### **A. SGD (Stochastic Gradient Descent)**

```python
weights_new = weights_old - learning_rate * gradient

# VÃ­ dá»¥:
weight = 0.5
gradient = 0.2  # Direction to move
learning_rate = 0.01

weight_new = 0.5 - 0.01 * 0.2
           = 0.498
```

---

### **B. Adam** â­ PHá»” BIáº¾N NHáº¤T (dÃ¹ng trong dá»± Ã¡n)

**Adam = SGD + Momentum + Adaptive LR**

```python
# Adam tá»± Ä‘á»™ng adjust learning rate cho tá»«ng parameter
# â†’ Há»c nhanh hÆ¡n, á»•n Ä‘á»‹nh hÆ¡n SGD

# Config trong dá»± Ã¡n:
optimizer = Adam(
    learning_rate=0.001,  # Initial LR
    beta_1=0.9,           # Momentum
    beta_2=0.999          # RMSprop
)
```

**Táº¡i sao dÃ¹ng Adam?**
- âœ… Tá»± Ä‘á»™ng adjust LR
- âœ… Faster convergence
- âœ… Work well vá»›i CNN

---

## ğŸ“Š 7. OVERFITTING VS UNDERFITTING (10 phÃºt)

### **A. Underfitting (Há»c chÆ°a Ä‘á»§)**

```
Train Acc: 60%
Val Acc:   58%

â†’ Model quÃ¡ Ä‘Æ¡n giáº£n, chÆ°a há»c Ä‘á»§ pattern
```

**Giáº£i phÃ¡p:**
- âœ… TÄƒng model capacity (more layers/neurons)
- âœ… Train lÃ¢u hÆ¡n (more epochs)

---

### **B. Overfitting (Há»c quÃ¡ ká»¹)**

```
Train Acc: 95%
Val Acc:   70%  â† GAP lá»›n!

â†’ Model nhá»› training data, khÃ´ng generalize
```

**Giáº£i phÃ¡p:**
- âœ… Data Augmentation
- âœ… Dropout
- âœ… Early Stopping
- âœ… Regularization

**Trong dá»± Ã¡n:**
```python
# Baseline CNN sá»­ dá»¥ng:
model.add(Dropout(0.5))  # Dropout 50%
model.add(BatchNormalization())

# Callbacks:
EarlyStopping(patience=5)  # Stop náº¿u val_loss khÃ´ng giáº£m
```

---

### **C. Good Fit (Vá»«a Ä‘á»§)** â­ Má»¤C TIÃŠU

```
Train Acc: 94%
Val Acc:   93%  â† Gap nhá»!

â†’ Model generalize tá»‘t!
```

**Trong dá»± Ã¡n:**
```
MobileNetV2:
  Train Acc: ~95%
  Val Acc: 94.00%
  Test Acc: 93.90%

â†’ EXCELLENT FIT! âœ…
```

---

## ğŸ“ Tá»”NG Káº¾T

### **Concepts quan trá»ng:**

1. **Neural Network** = Layers of neurons
2. **Forward Prop** = Input â†’ Output
3. **Loss** = Äá»™ Ä‘o sai láº§m
4. **Backprop** = Há»c tá»« loss
5. **Optimizer** = Update weights (Adam)
6. **Overfitting** = Train tá»‘t, Val kÃ©m

### **Workflow:**

```
1. Load Data (images + labels)
   â†“
2. Forward Propagation (predict)
   â†“
3. Calculate Loss
   â†“
4. Backpropagation (gradients)
   â†“
5. Update Weights (optimizer)
   â†“
6. Repeat for all data (1 epoch)
   â†“
7. Repeat epochs until converge
```

---

## âœ… CHECKPOINT

**Báº¡n cáº§n hiá»ƒu Ä‘Æ°á»£c:**

- [ ] Neural Network cÃ³ layers, neurons
- [ ] Forward Prop tÃ­nh output tá»« input
- [ ] Loss Ä‘o Ä‘á»™ sai láº§m
- [ ] Backprop cáº­p nháº­t weights Ä‘á»ƒ giáº£m loss
- [ ] Adam optimizer tá»‘t hÆ¡n SGD
- [ ] Overfitting vs Underfitting

**Náº¿u OK â†’** Tiáº¿p tá»¥c `02_CNN_Va_Computer_Vision.md` ğŸš€
