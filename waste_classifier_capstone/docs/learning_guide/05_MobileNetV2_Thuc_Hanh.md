# ğŸ“± MOBILENETV2 THá»°C HÃ€NH

**Thá»i gian:** 1.5 giá»
**Má»¥c tiÃªu:** Hiá»ƒu MobileNetV2 architecture vÃ  implementation trong dá»± Ã¡n

---

## ğŸ“Œ 1. MOBILENETV2 LÃ€ GÃŒ? (10 phÃºt)

### **Äá»‹nh nghÄ©a:**

**MobileNetV2 = CNN Ä‘Æ°á»£c thiáº¿t káº¿ cho MOBILE DEVICES**

```
Má»¥c tiÃªu:
âœ… Nháº¹ (lightweight) - Ãt parameters
âœ… Nhanh (fast) - Inference nhanh
âœ… ChÃ­nh xÃ¡c (accurate) - Accuracy cao
âœ… Efficient - Tiáº¿t kiá»‡m memory, power

â†’ Perfect cho deployment lÃªn phone, Raspberry Pi, edge devices!
```

### **Specs:**

```
Architecture:
  - 53 layers (deep!)
  - 3.5M parameters (lightweight!)
  - ImageNet Top-1: 72.0%
  - Latency: ~75ms on mobile CPU

Trong dá»± Ã¡n:
  - Pretrained on ImageNet
  - Fine-tuned for waste classification
  - Final accuracy: 93.90%
  - Model size: 25 MB (Keras), 9.8 MB (TFLite FP32)
```

---

## ğŸ” 2. KEY INNOVATIONS (20 phÃºt)

### **A. Depthwise Separable Convolution**

**Problem vá»›i Standard Conv:**

```
Standard Convolution (Baseline CNN):
  Input: [H, W, C_in]
  Filters: F filters of size [K, K, C_in]
  Output: [H, W, C_out=F]

  Parameters: K Ã— K Ã— C_in Ã— C_out

Example:
  Input: [56, 56, 64]
  Filters: 128 filters of [3, 3, 64]
  Parameters: 3 Ã— 3 Ã— 64 Ã— 128 = 73,728 params

  âœ— NHIá»€U parameters!
  âœ— CHáº¬M computation!
```

**Solution: Depthwise Separable Conv**

```
Depthwise Separable = Depthwise Conv + Pointwise Conv
```

**Step 1: Depthwise Convolution**

```
Depthwise Conv: Apply 1 filter PER INPUT CHANNEL

Input: [56, 56, 64]
Filters: 64 filters of [3, 3, 1]  â† 1 filter per channel!
Output: [56, 56, 64]  â† Same channels

Parameters: 3 Ã— 3 Ã— 1 Ã— 64 = 576 params

Visual:
Channel 1 â”€â”€[3x3 filter 1]â”€â”€â†’ Output Channel 1
Channel 2 â”€â”€[3x3 filter 2]â”€â”€â†’ Output Channel 2
...
Channel 64â”€â”€[3x3 filter 64]â”€â”€â†’ Output Channel 64

â†’ Spatial filtering ONLY (khÃ´ng mix channels)
```

**Step 2: Pointwise Convolution**

```
Pointwise Conv: 1Ã—1 conv to MIX CHANNELS

Input: [56, 56, 64]
Filters: 128 filters of [1, 1, 64]  â† 1Ã—1 size!
Output: [56, 56, 128]

Parameters: 1 Ã— 1 Ã— 64 Ã— 128 = 8,192 params

Visual:
[C1, C2, ..., C64] â”€â”€[1Ã—1 Conv]â”€â”€â†’ Output Channel 1
[C1, C2, ..., C64] â”€â”€[1Ã—1 Conv]â”€â”€â†’ Output Channel 2
...
[C1, C2, ..., C64] â”€â”€[1Ã—1 Conv]â”€â”€â†’ Output Channel 128

â†’ Channel mixing ONLY (khÃ´ng spatial filtering)
```

**Comparison:**

```
Standard Conv:
  Params: 3 Ã— 3 Ã— 64 Ã— 128 = 73,728

Depthwise Separable Conv:
  Depthwise:  3 Ã— 3 Ã— 1 Ã— 64  = 576
  Pointwise:  1 Ã— 1 Ã— 64 Ã— 128 = 8,192
  Total:                        8,768

Reduction: 73,728 / 8,768 = 8.4x fewer params! ğŸ”¥
           (Same computation reduction!)
```

**Why it works:**

```
Key Insight: Spatial filtering vÃ  Channel mixing lÃ  INDEPENDENT!

Standard Conv mixes both:
  âœ— Inefficient (redundant computations)

Depthwise Separable separates them:
  âœ“ Depthwise: Spatial filtering
  âœ“ Pointwise: Channel mixing
  âœ“ Much more efficient!
```

---

### **B. Inverted Residual Block**

**ResNet Residual Block (traditional):**

```
Input [256] â†’ Bottleneck Conv [64] â†’ Conv [64] â†’ Expand [256] â†’ Add
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         Skip connection

Pattern: Wide â†’ Narrow â†’ Wide (bottleneck in middle)
```

**MobileNetV2 Inverted Residual:**

```
Input [24] â†’ Expand [144] â†’ Depthwise [144] â†’ Project [24] â†’ Add
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      Skip connection

Pattern: Narrow â†’ Wide â†’ Narrow (inverted!)
```

**Detailed Steps:**

```
Step 1: Expansion (1Ã—1 Conv)
  Input: [56, 56, 24]
  Expand to: [56, 56, 144]  (6x expansion!)

  Purpose: Create high-dimensional space for feature learning

Step 2: Depthwise Conv (3Ã—3)
  Input: [56, 56, 144]
  Depthwise Conv: [56, 56, 144]
  ReLU6 activation

  Purpose: Spatial filtering in expanded space

Step 3: Projection (1Ã—1 Conv)
  Input: [56, 56, 144]
  Project to: [56, 56, 24]  (compress back)
  LINEAR activation (no ReLU!)

  Purpose: Compress back to low dimension

Step 4: Skip Connection
  IF input_channels == output_channels:
    output = input + projection_output
  ELSE:
    output = projection_output
```

**Why "Inverted"?**

```
Traditional Residual:
  256 â†’ [64] â†’ 256
  Wide â†’ Narrow â†’ Wide
  Bottleneck in MIDDLE

Inverted Residual:
  24 â†’ [144] â†’ 24
  Narrow â†’ Wide â†’ Narrow
  Expansion in MIDDLE (inverted!)
```

**Benefits:**

```
âœ… Memory efficient:
   - Input/Output: Narrow (24 channels)
   - Intermediate: Wide (144 channels)
   - Skip connection: Only 24 channels (cheap!)

âœ… Expressiveness:
   - Expansion creates rich representation
   - Depthwise in high dimension = more features

âœ… Linear Bottleneck:
   - Last layer = Linear (no ReLU)
   - Preserve information (ReLU kills negatives!)
```

---

### **C. Linear Bottleneck**

**Why NO ReLU in last layer?**

```
Problem with ReLU in low dimension:

ReLU(x) = max(0, x)
  â†’ Kills all negative values
  â†’ In low dimension (24 channels), information LOSS!

Example:
  Before ReLU: [-0.5, 0.8, -0.2, 0.3, ...]  (24 values)
  After ReLU:  [0.0,  0.8, 0.0,  0.3, ...]  (lost 2/4 values!)

  â†’ Information lost permanently!

In high dimension (144 channels):
  Before ReLU: 144 values
  After ReLU: ~72 values become 0

  â†’ Still have 72 non-zero values
  â†’ Less information loss (redundancy)

Solution: LINEAR bottleneck
  â†’ No ReLU in projection layer
  â†’ Preserve ALL information!
```

---

## ğŸ—ï¸ 3. MOBILENETV2 ARCHITECTURE (20 phÃºt)

### **A. Overall Structure**

```
MobileNetV2 (53 layers):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: [224, 224, 3]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv2D 3Ã—3, 32 filters, stride=2    â”‚ â† Initial conv
â”‚ Output: [112, 112, 32]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inverted Residual Block Ã— 1         â”‚
â”‚ t=1, c=16, n=1, s=1                 â”‚
â”‚ Output: [112, 112, 16]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inverted Residual Block Ã— 2         â”‚
â”‚ t=6, c=24, n=2, s=2                 â”‚
â”‚ Output: [56, 56, 24]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inverted Residual Block Ã— 3         â”‚
â”‚ t=6, c=32, n=3, s=2                 â”‚
â”‚ Output: [28, 28, 32]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inverted Residual Block Ã— 4         â”‚
â”‚ t=6, c=64, n=4, s=2                 â”‚
â”‚ Output: [14, 14, 64]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inverted Residual Block Ã— 3         â”‚
â”‚ t=6, c=96, n=3, s=1                 â”‚
â”‚ Output: [14, 14, 96]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inverted Residual Block Ã— 3         â”‚
â”‚ t=6, c=160, n=3, s=2                â”‚
â”‚ Output: [7, 7, 160]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inverted Residual Block Ã— 1         â”‚
â”‚ t=6, c=320, n=1, s=1                â”‚
â”‚ Output: [7, 7, 320]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv2D 1Ã—1, 1280 filters            â”‚ â† Final conv
â”‚ Output: [7, 7, 1280]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legend:
  t = expansion factor (expansion ratio)
  c = output channels
  n = number of repeats
  s = stride (first block only)
```

**Parameters:**
```
Total: 3.5M (ImageNet version)
Trong dá»± Ã¡n: 2.7M (custom classifier)
```

---

### **B. Receptive Field**

```
MobileNetV2 Receptive Field: ~150Ã—150 pixels (70% of image)

Comparison:
  Baseline:    61Ã—61   (27%)  â† Too small!
  MobileNetV2: 150Ã—150 (70%)  â† Much better!

Visualization:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚â”‚ [Plastic Bottle] â”‚  â”‚ â† MobileNetV2 sees FULL object!
â”‚â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚â”‚ â”‚ Cap  â”‚         â”‚  â”‚
â”‚â”‚ â”‚ Body â”‚         â”‚  â”‚
â”‚â”‚ â”‚Label â”‚         â”‚  â”‚
â”‚â”‚ â””â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» 4. CODE WALKTHROUGH (30 phÃºt)

### **File: src/models/transfer.py**

#### **Function 1: build_transfer_model**

```python
def build_transfer_model(input_shape, num_classes, freeze_base=True):
    """
    Build MobileNetV2 transfer learning model.

    Arguments:
    input_shape: (224, 224, 3)
    num_classes: 10 (waste classes)
    freeze_base: True for Phase 1, False for Phase 2

    Returns:
    model: Keras Model
    """

    # ===== STEP 1: Load Pretrained Base =====
    base_model = keras.applications.MobileNetV2(
        input_shape=input_shape,      # (224, 224, 3)
        include_top=False,             # Remove ImageNet classifier
        weights='imagenet'             # Load pretrained weights
    )

    # Base model layers:
    #   - 154 layers total
    #   - Includes all inverted residual blocks
    #   - Output: [7, 7, 1280]

    # ===== STEP 2: Freeze Base Model =====
    base_model.trainable = not freeze_base

    # freeze_base=True (Phase 1):
    #   â†’ All 154 layers FROZEN
    #   â†’ Only train new classifier

    # freeze_base=False (Phase 2):
    #   â†’ All 154 layers TRAINABLE
    #   â†’ Will freeze selectively later

    # ===== STEP 3: Build Complete Model =====
    inputs = keras.Input(shape=input_shape)

    # Preprocessing for MobileNetV2
    # Converts [0, 255] â†’ [-1, 1]
    x = keras.applications.mobilenet_v2.preprocess_input(inputs)

    # IMPORTANT: training argument for BatchNorm
    # Phase 1 (frozen): training=False â†’ Use pretrained BN stats
    # Phase 2 (fine-tune): training=True â†’ Update BN stats
    x = base_model(x, training=not freeze_base)

    # Output from base: [7, 7, 1280]

    # ===== STEP 4: Classification Head =====
    # Global Average Pooling
    x = layers.GlobalAveragePooling2D(name="GlobalAvgPool")(x)
    # [7, 7, 1280] â†’ [1280]

    # Dense Layer 1
    x = layers.Dense(256, activation='relu', name="Dense_1")(x)
    # [1280] â†’ [256]
    x = layers.BatchNormalization(name="BatchNorm_1")(x)
    x = layers.Dropout(0.3, name="Dropout_1")(x)

    # Dense Layer 2 (deeper head for more capacity)
    x = layers.Dense(128, activation='relu', name="Dense_2")(x)
    # [256] â†’ [128]
    x = layers.BatchNormalization(name="BatchNorm_2")(x)
    x = layers.Dropout(0.3, name="Dropout_2")(x)

    # Output Layer
    outputs = layers.Dense(num_classes, activation='softmax', name="Classifier")(x)
    # [128] â†’ [10]

    # ===== STEP 5: Create Model =====
    model = keras.Model(inputs, outputs, name="MobileNetV2_Transfer_Learning")

    return model
```

**Model Summary:**

```
Total parameters:    2,753,930
Trainable (Phase 1): 428,298  (15.5%) â† Only classifier
Frozen (Phase 1):    2,325,632 (84.5%) â† Base model
```

---

#### **Function 2: unfreeze_layers**

```python
def unfreeze_layers(model, num_layers_to_unfreeze):
    """
    Unfreeze top N layers for Phase 2 fine-tuning.

    Example: unfreeze_layers(model, 54)
    â†’ Unfreeze last 54 layers (top ~35% of 154 layers)
    """

    # Get base model (actual name from Keras)
    base_model = model.get_layer('mobilenetv2_1.00_224')
    base_model.trainable = True

    # First, freeze ALL layers
    for layer in base_model.layers:
        layer.trainable = False

    # Then, unfreeze top N layers
    for layer in base_model.layers[-num_layers_to_unfreeze:]:
        layer.trainable = True

    print(f"Unfroze {num_layers_to_unfreeze} layers from base model.")

    # Example: unfreeze_layers(model, 54)
    # Frozen: layers[0:100]   (early and middle)
    # Trainable: layers[100:154] (late layers)

    return model
```

**Layer Freezing Strategy:**

```
Total 154 layers in base_model:

Layers 0-99 (Early & Middle):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Generic features       â”‚ â† ALWAYS FROZEN
  â”‚ - Edges, textures      â”‚
  â”‚ - Basic patterns       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Layers 100-154 (Late):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ High-level features    â”‚ â† FROZEN in Phase 1
  â”‚ - Object parts         â”‚ â† TRAINABLE in Phase 2
  â”‚ - Semantic patterns    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **File: scripts/04_transfer_learning.py**

**Complete Training Flow:**

```python
# ===== PHASE 1: FEATURE EXTRACTION =====

print("PHASE 1: FEATURE EXTRACTION")

# 1. Build model with frozen base
model = build_transfer_model(
    input_shape=(224, 224, 3),
    num_classes=10,
    freeze_base=True  # â† FREEZE!
)

# 2. Compile with moderate LR
model.compile(
    optimizer=Adam(learning_rate=1e-4),  # 0.0001
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# 3. Callbacks
callbacks_phase1 = [
    EarlyStopping(patience=5, restore_best_weights=True),
    ModelCheckpoint('mobilenetv2_phase1.keras', save_best_only=True)
]

# 4. Train Phase 1
history_phase1 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=20,
    callbacks=callbacks_phase1
)

print(f"Phase 1 Result: {max(history_phase1.history['val_accuracy']):.2%}")

# ===== PHASE 2: FINE-TUNING =====

print("\nPHASE 2: FINE-TUNING")

# 5. Unfreeze top layers
model = unfreeze_layers(model, num_layers_to_unfreeze=54)

# 6. Recompile with VERY LOW LR
model.compile(
    optimizer=Adam(learning_rate=1e-5),  # 10x lower!
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# 7. Callbacks Phase 2
callbacks_phase2 = [
    EarlyStopping(patience=5, restore_best_weights=True),
    ModelCheckpoint('mobilenetv2_final.keras', save_best_only=True)
]

# 8. Train Phase 2
history_phase2 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=15,
    initial_epoch=20,  # Continue from Phase 1
    callbacks=callbacks_phase2
)

print(f"Phase 2 Result: {max(history_phase2.history['val_accuracy']):.2%}")

# ===== FINAL EVALUATION =====

test_loss, test_acc = model.evaluate(test_ds)
print(f"\nFinal Test Accuracy: {test_acc:.2%}")
```

---

## ğŸ“Š 5. TRAINING RESULTS (15 phÃºt)

### **A. Phase 1: Feature Extraction**

```
Configuration:
  - Frozen base: 2,325,632 params
  - Trainable classifier: 428,298 params
  - Learning rate: 1e-4
  - Epochs: 20

Results:
  Epoch 1:  Val Acc = 85.12%  â† Already good!
  Epoch 5:  Val Acc = 90.34%
  Epoch 10: Val Acc = 92.10%
  Epoch 15: Val Acc = 92.65%
  Epoch 20: Val Acc = 92.78%  â† Best

Training Time: ~30 minutes
```

**Why already good at Epoch 1?**

```
Pretrained features WORK WELL:
  âœ“ Edges, textures learned on ImageNet
  âœ“ Transferable to waste images
  âœ“ Classifier just needs to learn linear combination
  âœ“ Fast convergence!
```

---

### **B. Phase 2: Fine-Tuning**

```
Configuration:
  - Unfrozen late layers: 54 layers
  - Total trainable: 1,234,890 params
  - Learning rate: 1e-5 (very low!)
  - Epochs: 15

Results:
  Epoch 21: Val Acc = 92.95% (+0.17% from Phase 1)
  Epoch 25: Val Acc = 93.45% (+0.50%)
  Epoch 30: Val Acc = 93.78% (+0.33%)
  Epoch 35: Val Acc = 93.90% (+0.12%) â† BEST!

  EarlyStopping at Epoch 35 (patience=5)

Training Time: ~30 minutes
```

**Why Phase 2 improves?**

```
Fine-tuning adapts features to waste domain:
  âœ“ ImageNet: cats, dogs, cars
  âœ“ Waste: plastic, glass, metal
  âœ“ High-level features need adaptation
  âœ“ Phase 2 fine-tunes these features
  âœ“ Result: +1.12% improvement!
```

---

### **C. Final Results**

```
MobileNetV2 Transfer Learning:

Train Accuracy:      94.56%
Validation Accuracy: 94.00%
Test Accuracy:       93.90%

Train-Val Gap:       0.56%  â† Excellent generalization!

Comparison vs Baseline:
  Baseline:  79.51%
  MobileNet: 93.90%

  Improvement: +14.39 percentage points! ğŸ”¥
  Relative:    +18.1%
```

---

### **D. Per-Class Performance**

```
Class Performance (sorted by accuracy):

Top 3:
  1. clothes:   96.50%  âœ…
  2. shoes:     95.20%  âœ…
  3. cardboard: 94.10%  âœ…

Good (>90%):
  4. plastic:   93.40%
  5. paper:     92.80%
  6. biological:91.80%
  7. metal:     91.50%

Medium (85-90%):
  8. glass:     89.70%
  9. battery:   87.90%

Challenging:
  10. trash:    82.30%  â† Hardest (no clear pattern)

Average: 93.90%
```

**Why trash is hardest?**

```
Trash class = General waste
  âœ— No consistent visual pattern
  âœ— Mix of many materials
  âœ— Highly variable appearance
  âœ— Even humans struggle!

Other classes:
  âœ“ Consistent appearance
  âœ“ Clear material properties
  âœ“ Easier to classify
```

---

## ğŸ“ Tá»”NG Káº¾T

### **Key Innovations:**

1. **Depthwise Separable Conv** â†’ 8.4x fewer params than standard conv
2. **Inverted Residuals** â†’ Narrow-Wide-Narrow pattern
3. **Linear Bottleneck** â†’ Preserve information in low dimension
4. **Two-Phase Training** â†’ Feature extraction + Fine-tuning

### **Why MobileNetV2 is Better:**

```
vs Baseline CNN:

Architecture:
  Baseline: 8 layers, 1.4M params
  MobileNetV2: 53 layers, 2.7M params (but efficient!)

Receptive Field:
  Baseline: 61Ã—61 (27%)
  MobileNetV2: 150Ã—150 (70%)  â† Sees full object!

Pre-training:
  Baseline: None (random init)
  MobileNetV2: ImageNet (1.2M images)  â† Huge advantage!

Results:
  Baseline: 79.51%
  MobileNetV2: 93.90% (+14.39%!) ğŸ”¥
```

### **Model Comparison:**

| Metric | Baseline CNN | MobileNetV2 |
|--------|--------------|-------------|
| **Accuracy** | 79.51% | 93.90% |
| **Parameters** | 1.4M | 2.7M |
| **Layers** | 8 | 53 |
| **Receptive Field** | 61Ã—61 | 150Ã—150 |
| **Pretrained** | âœ— | âœ… ImageNet |
| **Training Time** | ~60 mins | ~60 mins |
| **Model Size** | ~5.6 MB | ~25 MB |
| **TFLite FP32** | - | 9.8 MB |
| **TFLite INT8** | - | 2.9 MB |

---

## âœ… CHECKPOINT

**Báº¡n cáº§n hiá»ƒu Ä‘Æ°á»£c:**

- [ ] MobileNetV2 designed for mobile/edge devices
- [ ] Depthwise Separable Conv = Depthwise + Pointwise
- [ ] 8.4x parameter reduction vs standard conv
- [ ] Inverted Residual: Narrow â†’ Wide â†’ Narrow
- [ ] Linear Bottleneck preserves information
- [ ] Two-phase training: frozen â†’ partial unfreeze
- [ ] Phase 1: 92.78%, Phase 2: 93.90% (+1.12%)
- [ ] Final result: 93.90% (+14.39% vs Baseline)

**Náº¿u OK â†’** Tiáº¿p tá»¥c `06_Optimization_Va_Deployment.md` ğŸš€
