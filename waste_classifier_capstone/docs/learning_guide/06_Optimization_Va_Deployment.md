# ğŸš€ OPTIMIZATION VÃ€ DEPLOYMENT

**Thá»i gian:** 1 giá»
**Má»¥c tiÃªu:** Hiá»ƒu cÃ¡ch optimize model vÃ  deploy lÃªn production

---

## ğŸ“Œ 1. Táº I SAO Cáº¦N OPTIMIZATION? (10 phÃºt)

### **Problem:**

```
Trained Model (Keras):
  File: mobilenetv2_final.keras
  Size: 25.0 MB
  Format: Keras SavedModel
  Precision: FP32 (32-bit floating point)

Deployment Targets:
  ğŸ“± Smartphone (Android/iOS)
  ğŸ¥§ Raspberry Pi
  ğŸ–¥ï¸ Edge devices (limited resources)

Challenges:
  âœ— 25 MB quÃ¡ lá»›n cho mobile apps
  âœ— FP32 inference cháº­m trÃªn mobile CPU
  âœ— Tá»‘n battery, memory
  âœ— KhÃ´ng phÃ¹ há»£p real-time applications
```

### **Solution: Model Optimization**

```
Original Model (Keras):
  Size: 25.0 MB
  Precision: FP32
  Inference: ~200ms (mobile CPU)

Optimized Model (TFLite):
  FP32:
    Size: 9.84 MB (-60.7%)  âœ…
    Accuracy: 93.90% (no loss!)
    Inference: ~100ms (-50%)

  INT8 (Quantized):
    Size: 2.94 MB (-88.3%!)  ğŸ”¥
    Accuracy: 93.20% (-0.70%)  â† Acceptable!
    Inference: ~50ms (-75%!)  â† VERY FAST!
```

---

## ğŸ”§ 2. TENSORFLOW LITE (TFLite) (15 phÃºt)

### **A. TFLite lÃ  gÃ¬?**

**TensorFlow Lite = Framework cho mobile & edge deployment**

```
TensorFlow:
  âœ“ Training models (powerful)
  âœ— Large size
  âœ— Desktop/server only

TensorFlow Lite:
  âœ“ Inference only (lightweight)
  âœ“ Optimized for mobile/edge
  âœ“ Small size, fast inference
  âœ— Cannot train models
```

**Platforms supported:**

```
Mobile:
  ğŸ“± Android (Java/Kotlin)
  ğŸ“± iOS (Swift/Obj-C)

Edge:
  ğŸ¥§ Raspberry Pi (Python)
  ğŸ”Œ Coral Edge TPU
  ğŸ® Jetson Nano

Web:
  ğŸŒ TensorFlow.js
```

---

### **B. Conversion Process**

```
Keras Model (.keras)
    â†“
[TFLite Converter]
    â†“
TFLite Model (.tflite)
    â†“
[Deploy]
    â†“
Mobile/Edge Device
```

**Code:**

```python
import tensorflow as tf

# 1. Load Keras model
model = tf.keras.models.load_model('mobilenetv2_final.keras')

# 2. Convert to TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# 3. Save TFLite model
with open('mobilenetv2_fp32.tflite', 'wb') as f:
    f.write(tflite_model)

print(f"TFLite model size: {len(tflite_model) / 1024 / 1024:.2f} MB")
```

---

### **C. TFLite Benefits**

```
1. Size Reduction:
   Keras: 25.0 MB
   TFLite: 9.84 MB
   â†’ 60.7% smaller!

   Why?
   âœ“ Remove training ops (backprop, etc.)
   âœ“ Optimize graph (fuse ops)
   âœ“ Compress weights

2. Speed Improvement:
   FP32 Keras: ~200ms
   FP32 TFLite: ~100ms
   â†’ 2x faster!

   Why?
   âœ“ Optimized kernels for mobile
   âœ“ Graph optimization
   âœ“ Hardware acceleration (NNAPI, GPU)

3. Memory Efficiency:
   âœ“ Smaller model â†’ Less RAM
   âœ“ Optimized inference â†’ Less peak memory
   âœ“ Better for resource-constrained devices
```

---

## âš¡ 3. QUANTIZATION (20 phÃºt)

### **A. Quantization lÃ  gÃ¬?**

**Quantization = Giáº£m precision cá»§a weights vÃ  activations**

```
Standard Model (FP32):
  Weights: 32-bit floating point
  Range: Â±3.4 Ã— 10^38
  Precision: ~7 decimal digits

  Example weight: 0.123456789 (32 bits)

Quantized Model (INT8):
  Weights: 8-bit integer
  Range: -128 to 127
  Precision: 256 values

  Example weight: 31 (8 bits)
  â†’ Maps to ~0.123 after dequantization

Reduction: 32 bits â†’ 8 bits = 4x smaller!
```

---

### **B. How Quantization Works**

**Linear Quantization Formula:**

```
Quantization (FP32 â†’ INT8):
  Q = round((F - zero_point) / scale)

  Where:
    F = FP32 value
    Q = INT8 value
    scale = (F_max - F_min) / 255
    zero_point = INT8 value for F = 0

Example:
  FP32 range: [-1.0, 1.0]
  scale = (1.0 - (-1.0)) / 255 = 0.00784
  zero_point = 0

  Quantize 0.5:
    Q = round(0.5 / 0.00784) = round(63.8) = 64

  Quantize -0.3:
    Q = round(-0.3 / 0.00784) = round(-38.3) = -38
```

**Dequantization (INT8 â†’ FP32):**

```
Dequantization:
  F = scale * (Q - zero_point)

Example:
  Q = 64
  F = 0.00784 * 64 = 0.5  âœ“

  Q = -38
  F = 0.00784 * -38 = -0.298 â‰ˆ -0.3  âœ“ (small error)
```

---

### **C. Types of Quantization**

#### **1. Post-Training Quantization (PTQ)**

```
Post-Training Quantization = Quantize SAU KHI train xong

Process:
  1. Train model vá»›i FP32 (normal)
  2. Model trained â†’ 93.90% accuracy
  3. Convert to INT8 using TFLite converter
  4. TFLite model â†’ 93.20% accuracy (-0.70%)

Pros:
  âœ… Easy (no retraining)
  âœ… Fast (minutes to convert)
  âœ… Good accuracy (usually <1% loss)

Cons:
  âš  Slight accuracy drop
  âš  May need calibration data
```

**Code:**

```python
# Post-Training INT8 Quantization

# 1. Load model
model = tf.keras.models.load_model('mobilenetv2_final.keras')

# 2. Create converter
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# 3. Enable INT8 quantization
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# 4. Provide representative dataset for calibration
def representative_dataset():
    for _ in range(100):
        # Sample from training data
        data = np.random.rand(1, 224, 224, 3).astype(np.float32)
        yield [data]

converter.representative_dataset = representative_dataset

# 5. Set INT8 input/output (full integer model)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

# 6. Convert
tflite_model = converter.convert()

# 7. Save
with open('mobilenetv2_int8.tflite', 'wb') as f:
    f.write(tflite_model)
```

---

#### **2. Quantization-Aware Training (QAT)**

```
Quantization-Aware Training = Train model Vá»šI quantization simulation

Process:
  1. Insert fake quantization ops during training
  2. Model learns to work with quantized weights
  3. Convert to INT8 after training
  4. Minimal accuracy loss (<0.5%)

Pros:
  âœ… Better accuracy than PTQ
  âœ… Model adapts to quantization

Cons:
  âš  Longer training time
  âš  More complex implementation
```

**Dá»± Ã¡n nÃ y dÃ¹ng PTQ (Ä‘Æ¡n giáº£n hÆ¡n, Ä‘á»§ tá»‘t!)**

---

### **D. Calibration Dataset**

**Táº¡i sao cáº§n calibration?**

```
Problem:
  Quantization cáº§n biáº¿t FP32 range Ä‘á»ƒ compute scale

  Example:
    Layer 1 weights: [-0.5, 0.8]
    Layer 2 weights: [-2.3, 1.9]

    â†’ Different ranges!
    â†’ Need to measure ACTUAL ranges during inference

Solution: Calibration
  1. Run representative data through model
  2. Measure min/max values at each layer
  3. Compute optimal scale/zero_point
  4. Quantize with these parameters
```

**Code:**

```python
def representative_dataset():
    """
    Generate calibration data from training set.

    Should cover:
    - All classes
    - Various lighting conditions
    - Different object sizes
    """
    for images, _ in train_ds.take(100):  # 100 batches
        for img in images:
            # Yield single image
            yield [np.expand_dims(img, axis=0)]

converter.representative_dataset = representative_dataset
```

---

## ğŸ“Š 4. OPTIMIZATION RESULTS (10 phÃºt)

### **A. Size Comparison**

```
Original Keras Model:
  mobilenetv2_final.keras
  Size: 25.0 MB
  Precision: FP32

TFLite FP32:
  mobilenetv2_fp32.tflite
  Size: 9.84 MB
  Reduction: 60.7%
  Accuracy: 93.90% (NO LOSS!)

TFLite INT8:
  mobilenetv2_int8.tflite
  Size: 2.94 MB
  Reduction: 88.3%!
  Accuracy: 93.20% (-0.70%)

Visualization:
Keras    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 25.0 MB
TFLite   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  9.8 MB
INT8     â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  2.9 MB
```

---

### **B. Accuracy Comparison**

```
Test Set Evaluation (1,974 images):

Keras FP32:     93.90% â† Baseline
TFLite FP32:    93.90% â† NO LOSS!
TFLite INT8:    93.20% â† -0.70%

Per-Class Comparison (INT8 vs Keras):

clothes:   96.50% â†’ 96.30% (-0.20%)
shoes:     95.20% â†’ 95.00% (-0.20%)
cardboard: 94.10% â†’ 93.80% (-0.30%)
plastic:   93.40% â†’ 92.90% (-0.50%)
paper:     92.80% â†’ 92.50% (-0.30%)
biological:91.80% â†’ 91.20% (-0.60%)
metal:     91.50% â†’ 91.00% (-0.50%)
glass:     89.70% â†’ 88.90% (-0.80%)  â† Largest drop
battery:   87.90% â†’ 87.10% (-0.80%)
trash:     82.30% â†’ 81.50% (-0.80%)

â†’ Accuracy drop CONSISTENT across classes
â†’ No catastrophic failures
â†’ Trade-off: 88.3% size reduction for 0.70% accuracy
```

---

### **C. Inference Speed**

```
Platform: Raspberry Pi 4 (4GB RAM)

Keras FP32:
  Inference: ~200ms/image
  FPS: 5

TFLite FP32:
  Inference: ~100ms/image
  FPS: 10  (2x faster!)

TFLite INT8:
  Inference: ~50ms/image
  FPS: 20  (4x faster!)

Platform: Android Phone (mid-range)

Keras FP32:
  Not supported (too large)

TFLite FP32:
  Inference: ~80ms/image
  FPS: 12

TFLite INT8 (with NNAPI):
  Inference: ~30ms/image
  FPS: 33  (real-time!)
```

---

## ğŸ¯ 5. DEPLOYMENT SCENARIOS (15 phÃºt)

### **A. Raspberry Pi Deployment**

**Use case:** Waste sorting machine

```python
import tflite_runtime.interpreter as tflite
import numpy as np
from PIL import Image

# 1. Load TFLite model
interpreter = tflite.Interpreter(
    model_path='mobilenetv2_int8.tflite'
)
interpreter.allocate_tensors()

# 2. Get input/output details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# 3. Load and preprocess image
image = Image.open('waste.jpg').resize((224, 224))
input_data = np.array(image, dtype=np.uint8)  # INT8 model expects uint8
input_data = np.expand_dims(input_data, axis=0)

# 4. Run inference
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output = interpreter.get_tensor(output_details[0]['index'])[0]

# 5. Dequantize output (INT8 â†’ probabilities)
scale, zero_point = output_details[0]['quantization']
output = scale * (output.astype(np.float32) - zero_point)

# 6. Get prediction
class_idx = np.argmax(output)
confidence = output[class_idx]

print(f"Prediction: {CLASS_NAMES[class_idx]}")
print(f"Confidence: {confidence:.2%}")
```

**Hardware requirements:**

```
Raspberry Pi 4:
  CPU: Quad-core ARM Cortex-A72
  RAM: 2GB+ recommended
  Storage: 8GB+ SD card
  Camera: Raspberry Pi Camera Module v2

Performance:
  INT8 model: ~50ms/image
  FPS: 20 (sufficient for sorting!)
```

---

### **B. Mobile App Deployment**

**Android Example:**

```kotlin
// 1. Load TFLite model from assets
private val tflite = Interpreter(loadModelFile())

// 2. Prepare input
val inputBuffer = ByteBuffer.allocateDirect(1 * 224 * 224 * 3)
inputBuffer.order(ByteOrder.nativeOrder())

// Fill buffer with image pixels (uint8)
bitmap.getPixels(pixels, 0, 224, 0, 0, 224, 224)
for (pixel in pixels) {
    inputBuffer.put((pixel shr 16 and 0xFF).toByte())  // R
    inputBuffer.put((pixel shr 8 and 0xFF).toByte())   // G
    inputBuffer.put((pixel and 0xFF).toByte())         // B
}

// 3. Prepare output
val outputBuffer = ByteBuffer.allocateDirect(10)  // 10 classes

// 4. Run inference
tflite.run(inputBuffer, outputBuffer)

// 5. Get prediction
val probabilities = FloatArray(10)
outputBuffer.rewind()
for (i in 0..9) {
    // Dequantize
    val q = outputBuffer.get().toInt() and 0xFF
    probabilities[i] = scale * (q - zeroPoint)
}

val prediction = probabilities.indices.maxByOrNull { probabilities[it] }!!
```

**App size impact:**

```
Without TFLite model:
  APK size: ~10 MB

With Keras model (not possible):
  APK size: ~35 MB (too large!)

With TFLite FP32:
  APK size: ~20 MB (acceptable)

With TFLite INT8:
  APK size: ~13 MB (excellent!)
```

---

### **C. Cloud vs Edge Deployment**

**Comparison:**

| Aspect | Cloud | Edge (TFLite) |
|--------|-------|---------------|
| **Latency** | 100-500ms | 30-100ms |
| **Internet** | Required | Not required |
| **Privacy** | Data sent to cloud | Data stays local |
| **Cost** | API calls fee | One-time hardware |
| **Scalability** | Easy | Limited by device |
| **Offline** | âœ— | âœ… |
| **Real-time** | âš  Depends | âœ… |

**When to use Edge (TFLite):**

```
âœ… Real-time requirements (sorting machine)
âœ… Privacy concerns (medical, personal)
âœ… Offline environments (remote areas)
âœ… Low latency critical (<100ms)
âœ… Cost-sensitive (avoid API fees)

Example: Waste sorting kiosk
  - Users drop waste
  - Camera captures image
  - TFLite model classifies (50ms)
  - Display result immediately
  - No internet needed!
```

**When to use Cloud:**

```
âœ… Complex models (too large for mobile)
âœ… Frequent updates needed
âœ… High accuracy critical
âœ… Centralized data collection

Example: Waste analytics platform
  - Users upload photos
  - Cloud processes with large model
  - Store results in database
  - Generate analytics reports
```

---

## ğŸ“ Tá»”NG Káº¾T

### **Key Concepts:**

1. **TensorFlow Lite** = Framework for mobile/edge deployment
2. **Quantization** = FP32 â†’ INT8 (4x smaller, 4x faster)
3. **Post-Training Quantization** = Quantize after training
4. **Calibration** = Measure activation ranges for optimal quantization

### **Optimization Results:**

```
Keras Model:
  Size: 25.0 MB
  Accuracy: 93.90%
  Inference: ~200ms

TFLite INT8:
  Size: 2.94 MB (-88.3%!)
  Accuracy: 93.20% (-0.70%)
  Inference: ~50ms (-75%!)

Trade-off: Excellent!
  â†’ Huge size/speed gain
  â†’ Minimal accuracy loss
```

### **Deployment:**

```
Best for Mobile/Edge:
  âœ… TFLite INT8 (2.94 MB)
  âœ… 20 FPS on Raspberry Pi
  âœ… 33 FPS on Android (NNAPI)
  âœ… Offline capable
  âœ… Privacy-preserving
```

---

## âœ… CHECKPOINT

**Báº¡n cáº§n hiá»ƒu Ä‘Æ°á»£c:**

- [ ] TFLite lÃ  framework cho mobile/edge deployment
- [ ] Quantization giáº£m FP32 â†’ INT8 (4x smaller)
- [ ] Post-Training Quantization quantize sau khi train
- [ ] Calibration dataset cáº§n Ä‘á»ƒ compute scale
- [ ] TFLite INT8: 2.94 MB, 93.20% accuracy
- [ ] 88.3% size reduction, 0.70% accuracy loss
- [ ] Inference 4x nhanh hÆ¡n trÃªn mobile
- [ ] Suitable cho Raspberry Pi, Android, iOS

**Náº¿u OK â†’** Tiáº¿p tá»¥c `07_Tong_Ket_Va_On_Tap.md` ğŸš€
