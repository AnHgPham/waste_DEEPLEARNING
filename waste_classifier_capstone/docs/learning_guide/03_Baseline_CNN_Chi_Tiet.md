# ğŸ—ï¸ BASELINE CNN CHI TIáº¾T

**Thá»i gian:** 1 giá»
**Má»¥c tiÃªu:** Hiá»ƒu code Baseline CNN trong dá»± Ã¡n vÃ  cÃ¡ch nÃ³ hoáº¡t Ä‘á»™ng

---

## ğŸ“Œ 1. BASELINE CNN LÃ€ GÃŒ? (5 phÃºt)

### **Äá»‹nh nghÄ©a:**

**Baseline CNN = Model Ä‘Æ¡n giáº£n Ä‘á»ƒ lÃ m baseline (Ä‘iá»ƒm chuáº©n)**

```
Má»¥c Ä‘Ã­ch:
1. âœ… Táº¡o baseline Ä‘á»ƒ so sÃ¡nh vá»›i models khÃ¡c
2. âœ… Há»c tá»« Ä‘áº§u (train from scratch) trÃªn waste data
3. âœ… KhÃ´ng dÃ¹ng pretrained weights
4. âœ… Kiá»ƒm tra dataset cÃ³ Ä‘á»§ tá»‘t khÃ´ng

Káº¿t quáº£:
- Train Acc: 81.28%
- Val Acc: 79.59%
- Test Acc: 79.51%
â†’ BASELINE Ä‘á»ƒ so sÃ¡nh!
```

### **Trong dá»± Ã¡n nÃ y:**

```python
# File: src/models/baseline.py
def build_baseline_model(input_shape, num_classes):
    # Build model tá»« Ä‘áº§u
    # Input: (224, 224, 3)
    # Output: (10,) - 10 waste classes
```

---

## ğŸ›ï¸ 2. ARCHITECTURE CHI TIáº¾T (20 phÃºt)

### **A. Tá»•ng Quan Architecture**

```
Input Image (224x224x3)
    â†“
[Rescaling Layer]        # [0, 255] â†’ [0, 1]
    â†“
[Conv Block 1]           # 32 filters
    â†“
[Conv Block 2]           # 64 filters
    â†“
[Conv Block 3]           # 128 filters
    â†“
[Conv Block 4]           # 256 filters
    â†“
[GlobalAveragePooling2D] # Flatten
    â†“
[Dense 128]              # Classification head
    â†“
[Dropout 0.5]            # Regularization
    â†“
[Dense 10]               # Output
    â†“
Softmax â†’ [10 probabilities]
```

---

### **B. Layer-by-Layer Breakdown**

#### **Layer 0: Rescaling (Chuáº©n hÃ³a)**

```python
model.add(layers.Rescaling(1./255))
```

**Má»¥c Ä‘Ã­ch:** Chuyá»ƒn pixel values tá»« [0, 255] â†’ [0, 1]

**Táº¡i sao?**
```python
# TRÆ¯á»šC rescaling:
pixel = 255  # White pixel
â†’ Neural network nháº­n input Lá»šN (255)
â†’ Weights cáº§n Lá»šN Ä‘á»ƒ há»c
â†’ Training KHÃ”NG á»•n Ä‘á»‹nh!

# SAU rescaling:
pixel = 255 / 255 = 1.0  # White pixel
â†’ Neural network nháº­n input NHá» (0-1)
â†’ Training á»•n Ä‘á»‹nh hÆ¡n
â†’ Gradients khÃ´ng explode!
```

**VÃ­ dá»¥:**
```python
Original Image:
[255, 128, 0]  # Red-ish pixel

After Rescaling:
[1.0, 0.5, 0.0]  # Same red, normalized
```

---

#### **Convolutional Blocks (4 blocks)**

**Cáº¥u trÃºc Má»–I block:**

```python
# Pseudo-code cho 1 block vá»›i N filters:
Conv2D(N, 3x3, ReLU, padding='same')  # First conv
Conv2D(N, 3x3, ReLU, padding='same')  # Second conv
BatchNormalization()                   # Normalize
MaxPooling2D(2x2)                      # Downsample
```

**Code thá»±c táº¿:**

```python
# From config.py:
BASELINE_FILTERS = [32, 64, 128, 256]

# From baseline.py:
for filters in BASELINE_FILTERS:
    model.add(layers.Conv2D(filters, (3, 3), activation='relu', padding='same'))
    model.add(layers.Conv2D(filters, (3, 3), activation='relu', padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
```

**Chi tiáº¿t tá»«ng block:**

##### **Block 1: 32 filters**

```
Input: [224, 224, 3]

Conv2D(32, 3x3, ReLU, same) â†’ [224, 224, 32]
  â†“ Há»c 32 filters (edge detectors, color detectors)
Conv2D(32, 3x3, ReLU, same) â†’ [224, 224, 32]
  â†“ Refine features
BatchNormalization() â†’ [224, 224, 32]
  â†“ Normalize Ä‘á»ƒ training á»•n Ä‘á»‹nh
MaxPooling2D(2x2) â†’ [112, 112, 32]
  â†“ Giáº£m kÃ­ch thÆ°á»›c xuá»‘ng 1/2
```

**Filters há»c gÃ¬?**
```
Filter 1: Vertical edges   (phÃ¡t hiá»‡n cáº¡nh dá»c)
Filter 2: Horizontal edges (phÃ¡t hiá»‡n cáº¡nh ngang)
Filter 3: Red color        (phÃ¡t hiá»‡n mÃ u Ä‘á»)
Filter 4: Blue color       (phÃ¡t hiá»‡n mÃ u xanh)
...
Filter 32: Complex patterns
```

---

##### **Block 2: 64 filters**

```
Input: [112, 112, 32]

Conv2D(64, 3x3, ReLU, same) â†’ [112, 112, 64]
  â†“ Há»c 64 patterns phá»©c táº¡p hÆ¡n
Conv2D(64, 3x3, ReLU, same) â†’ [112, 112, 64]
  â†“ Refine
BatchNormalization() â†’ [112, 112, 64]
  â†“ Normalize
MaxPooling2D(2x2) â†’ [56, 56, 64]
  â†“ Downsample
```

**Filters há»c gÃ¬?**
```
Combine low-level features tá»« Block 1:
- Texture patterns (nhÃ¡m, má»‹n)
- Simple shapes (circles, rectangles)
- Color combinations (plastic transparent)
```

---

##### **Block 3: 128 filters**

```
Input: [56, 56, 64]

Conv2D(128, 3x3, ReLU, same) â†’ [56, 56, 128]
Conv2D(128, 3x3, ReLU, same) â†’ [56, 56, 128]
BatchNormalization() â†’ [56, 56, 128]
MaxPooling2D(2x2) â†’ [28, 28, 128]
```

**Filters há»c gÃ¬?**
```
Mid-level features:
- Object parts (bottle cap, bottle body)
- Material textures (metal shine, glass clarity)
- Complex patterns
```

---

##### **Block 4: 256 filters**

```
Input: [28, 28, 128]

Conv2D(256, 3x3, ReLU, same) â†’ [28, 28, 256]
Conv2D(256, 3x3, ReLU, same) â†’ [28, 28, 256]
BatchNormalization() â†’ [28, 28, 256]
MaxPooling2D(2x2) â†’ [14, 14, 256]
```

**Filters há»c gÃ¬?**
```
High-level features:
- Whole objects (bottle, can, box)
- Semantic concepts (plastic-ness, metal-ness)
- Class-specific patterns
```

---

#### **GlobalAveragePooling2D**

```python
model.add(layers.GlobalAveragePooling2D())
```

**Má»¥c Ä‘Ã­ch:** Chuyá»ƒn 3D tensor â†’ 1D vector

```
Input: [14, 14, 256]  # Feature maps tá»« Block 4

Process:
  For each of 256 channels:
    Take average of all 14x14 pixels
    â†’ 1 number per channel

Output: [256,]  # 1D vector
```

**VÃ­ dá»¥:**

```python
# Channel 0 (vÃ­ dá»¥: "plastic detector"):
channel_0 = [
  [0.1, 0.2, ..., 0.3],  # 14x14 grid
  [0.4, 0.5, ..., 0.1],
  ...
]

average_0 = mean(channel_0) = 0.25
â†’ "Plastic confidence = 0.25"

# Repeat cho 256 channels
â†’ Output: [0.25, 0.82, 0.15, ..., 0.91]  # 256 numbers
```

**Táº¡i sao dÃ¹ng GAP?**
```
âœ… Giáº£m params (khÃ´ng cáº§n Dense layer lá»›n)
âœ… Spatial invariance (object á»Ÿ Ä‘Ã¢u cÅ©ng Ä‘Æ°á»£c)
âœ… Prevent overfitting
```

---

#### **Classification Head**

```python
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(num_classes, activation='softmax'))
```

**Dense 128:**

```
Input: [256,]  # From GAP

Dense(128, ReLU):
  output = ReLU(W @ input + b)
  â†’ [128,]

Má»¥c Ä‘Ã­ch: Káº¿t há»£p features Ä‘á»ƒ classify
```

**Dropout 0.5:**

```python
Dropout(0.5)
# Randomly set 50% neurons to 0 during training

Example:
Before: [0.5, 0.8, 0.3, 0.9, ...]
After:  [0.5, 0.0, 0.3, 0.0, ...]  # Random 50% dropped
```

**Táº¡i sao?**
```
âœ… Prevent overfitting
âœ… Force network to learn redundant representations
âœ… Improve generalization
```

**Chá»‰ dÃ¹ng khi training!**
```python
# Training mode:
model.fit(...) â†’ Dropout ACTIVE

# Inference mode:
model.predict(...) â†’ Dropout OFF (all neurons used)
```

**Dense 10 (Output):**

```
Input: [128,]

Dense(10, Softmax):
  logits = W @ input + b  â†’ [10,]  # Raw scores
  probs = Softmax(logits) â†’ [10,]  # Probabilities (sum=1)

Output:
[0.02, 0.01, 0.05, 0.02, 0.03, 0.01, 0.02, 0.82, 0.01, 0.01]
  â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘
 bat   bio   card cloth glass metal paper plas  shoes trash

Prediction: "plastic" (index 7, prob=0.82)
```

---

### **C. Model Summary**

```
Total Parameters: ~1.4M

Breakdown:
- Conv layers: ~1.2M params (majority)
- Dense layers: ~200K params
- BatchNorm: ~2K params

Receptive Field: ~61x61 pixels (27% of image)
```

**Compare vá»›i MobileNetV2:**

```
Baseline CNN:
  Params: 1.4M
  Depth: 8 conv layers
  Receptive Field: 61x61 (27%)
  Accuracy: 79.59%

MobileNetV2:
  Params: 2.7M
  Depth: 53 layers
  Receptive Field: 150x150 (70%)
  Accuracy: 93.90% (+14.31%!)
```

---

## ğŸ’» 3. CODE WALKTHROUGH (15 phÃºt)

### **File: src/models/baseline.py**

```python
def build_baseline_model(input_shape, num_classes):
    """
    Build Baseline CNN.

    Arguments:
    input_shape: (224, 224, 3)
    num_classes: 10 (waste classes)

    Returns:
    model: Compiled Keras model
    """

    # 1. Create Sequential model
    model = keras.Sequential(name="Baseline_CNN")
    model.add(layers.Input(shape=input_shape))

    # 2. CRITICAL: Rescale [0,255] â†’ [0,1]
    model.add(layers.Rescaling(1./255))

    # 3. Convolutional Blocks
    for filters in BASELINE_FILTERS:  # [32, 64, 128, 256]
        model.add(layers.Conv2D(filters, (3, 3), activation='relu', padding='same'))
        model.add(layers.Conv2D(filters, (3, 3), activation='relu', padding='same'))
        model.add(layers.BatchNormalization())
        model.add(layers.MaxPooling2D(pool_size=(2, 2)))

    # 4. Classifier Head
    model.add(layers.GlobalAveragePooling2D())
    model.add(layers.Dense(BASELINE_DENSE_UNITS, activation='relu'))  # 128
    model.add(layers.Dropout(BASELINE_DROPOUT_RATE))  # 0.5
    model.add(layers.Dense(num_classes, activation='softmax'))

    return model
```

**Giáº£i thÃ­ch tá»«ng bÆ°á»›c:**

**BÆ°á»›c 1: Create model**
```python
model = keras.Sequential(name="Baseline_CNN")
```
- Sequential = Layers xáº¿p tuáº§n tá»±
- Name = "Baseline_CNN" (Ä‘á»ƒ debug dá»…)

**BÆ°á»›c 2: Rescaling**
```python
model.add(layers.Rescaling(1./255))
```
- CRITICAL! KhÃ´ng cÃ³ layer nÃ y â†’ Training sáº½ fail
- Pixel [0, 255] â†’ [0, 1]

**BÆ°á»›c 3: Conv Blocks**
```python
for filters in [32, 64, 128, 256]:
    # 2x Conv + BN + MaxPool
```
- 4 blocks = 8 conv layers
- Filters tÄƒng dáº§n (32â†’256)
- Feature maps giáº£m dáº§n (224â†’14)

**BÆ°á»›c 4: Classification**
```python
GlobalAveragePooling2D()  # [14,14,256] â†’ [256]
Dense(128, ReLU)          # [256] â†’ [128]
Dropout(0.5)              # Regularization
Dense(10, Softmax)        # [128] â†’ [10]
```

---

### **File: scripts/03_baseline_training.py**

**Training process:**

```python
# 1. Load Data
train_ds = tf.keras.utils.image_dataset_from_directory(
    TRAIN_DIR,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='categorical'
)

# 2. Build Model
model = build_baseline_model(
    input_shape=INPUT_SHAPE,  # (224, 224, 3)
    num_classes=NUM_CLASSES    # 10
)

# 3. Compile
model.compile(
    optimizer=Adam(learning_rate=LEARNING_RATE_BASELINE),  # 0.001
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# 4. Callbacks
callbacks = [
    EarlyStopping(patience=5, restore_best_weights=True),
    ReduceLROnPlateau(patience=3, factor=0.5),
    ModelCheckpoint('baseline.keras', save_best_only=True)
]

# 5. Train
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=30,
    callbacks=callbacks
)

# 6. Evaluate
test_loss, test_acc = model.evaluate(test_ds)
print(f"Test Accuracy: {test_acc:.2%}")
```

---

## ğŸ“Š 4. TRAINING PROCESS (10 phÃºt)

### **A. Training History**

```
Epoch 1-5: FAST IMPROVEMENT
  Epoch 1:  Val Acc = 70.12%
  Epoch 5:  Val Acc = 76.34%

  Model Ä‘ang há»c:
  âœ“ Basic edges, colors
  âœ“ Simple shapes

Epoch 6-15: MODERATE IMPROVEMENT
  Epoch 10: Val Acc = 77.89%
  Epoch 15: Val Acc = 78.56%

  Model Ä‘ang há»c:
  âœ“ Textures (plastic smooth, metal shiny)
  âœ“ Mid-level patterns

Epoch 16-25: SLOW IMPROVEMENT
  Epoch 20: Val Acc = 79.12%
  Epoch 25: Val Acc = 79.41%

  Model struggling:
  âš  High-level features khÃ³ há»c
  âš  Model capacity gáº§n Ä‘áº¡t ceiling

Epoch 26-30: PLATEAU
  Epoch 26: Val Acc = 79.51%
  Epoch 30: Val Acc = 79.59%

  MODEL CEILING REACHED!
  â†’ KhÃ´ng cáº£i thiá»‡n thÃªm Ä‘Æ°á»£c
```

### **B. Learning Rate Schedule**

```
Initial LR: 0.001

ReduceLROnPlateau (patience=3, factor=0.5):
  Epoch 10: LR â†’ 0.0005  (val_loss khÃ´ng giáº£m 3 epochs)
  Epoch 18: LR â†’ 0.00025
  Epoch 25: LR â†’ 0.000125

Final LR: 0.000125
```

**Táº¡i sao reduce LR?**
```
High LR (0.001):
  â†’ Large weight updates
  â†’ Fast learning
  â†’ Coarse optimization

Low LR (0.0001):
  â†’ Small weight updates
  â†’ Slow learning
  â†’ Fine-tuning
```

---

## ğŸ“ˆ 5. Káº¾T QUáº¢ VÃ€ PHÃ‚N TÃCH (10 phÃºt)

### **A. Final Results**

```
Train Accuracy:      81.28%
Validation Accuracy: 79.59%
Test Accuracy:       79.51%

Gap (Train - Val):   1.69%  â† Small gap = Good generalization!
```

**Interpretation:**
```
âœ… Model generalize tá»‘t (gap nhá»)
âœ… KhÃ´ng overfitting nghiÃªm trá»ng
âš   Accuracy khÃ´ng cao (chá»‰ ~80%)
â†’ Cáº§n model tá»‘t hÆ¡n!
```

---

### **B. Per-Class Performance**

```
Easy Classes (>85%):
  âœ“ clothes:    94.10%  (Distinct texture & shape)
  âœ“ shoes:      89.90%  (Unique appearance)

Medium Classes (75-85%):
  âš  paper:      81.70%  (Confused with cardboard)
  âš  plastic:    78.30%  (Confused with glass)
  âš  metal:      76.20%  (Confused with foil)

Hard Classes (<75%):
  âœ— trash:      52.11%  (No clear pattern!)
  âœ— glass:      74.50%  (Confused with plastic)
```

**Pattern:**
```
Baseline handles DISTINCT classes well
  â†’ clothes, shoes cÃ³ appearance khÃ¡c biá»‡t

Baseline struggles with SIMILAR classes
  â†’ plastic vs glass (both transparent)
  â†’ paper vs cardboard (similar texture)
  â†’ trash (general waste, no pattern)
```

---

## ğŸ” 6. Táº I SAO CHá»ˆ Äáº T 79.59%? (15 phÃºt)

### **A. Model Capacity Limitation**

**1. Receptive Field QuÃ¡ Nhá»**

```
Baseline Receptive Field: 61x61 pixels (27% of image)

Plastic Bottle trong áº£nh:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      â”‚
â”‚    [Plastic Bottle]  â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚ Cap  â”‚          â”‚
â”‚    â”‚      â”‚          â”‚
â”‚    â”‚Label â”‚          â”‚
â”‚    â”‚      â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Baseline chá»‰ nhÃ¬n:
â”Œâ”€â”€â”€â”
â”‚Capâ”‚  â† Chá»‰ tháº¥y 1 pháº§n nhá»!
â””â”€â”€â”€â”˜

â†’ KhÃ´ng tháº¥y TOÃ€N Bá»˜ object
â†’ KhÃ³ classify Ä‘Ãºng!
```

**MobileNetV2 nhÃ¬n tháº¥y:**
```
Receptive Field: 150x150 pixels (70% of image)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Bottle]         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”         â”‚
â”‚ â”‚ Cap  â”‚         â”‚
â”‚ â”‚ Body â”‚         â”‚
â”‚ â”‚Label â”‚         â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â†’ Tháº¥y FULL object!
â†’ Classify tá»‘t hÆ¡n!
```

---

**2. Depth KhÃ´ng Äá»§**

```
Baseline: 8 conv layers
  â†’ Chá»‰ há»c Ä‘Æ°á»£c 3 levels of abstraction

  Level 1: Edges, colors
  Level 2: Textures, patterns
  Level 3: Simple shapes

  âœ— KHÃ”NG Há»ŒC ÄÆ¯á»¢C complex high-level features!

MobileNetV2: 53 layers
  â†’ Há»c Ä‘Æ°á»£c 6-7 levels

  Level 1: Edges
  Level 2: Textures
  Level 3: Object parts
  Level 4: Whole objects
  Level 5: Object relationships
  Level 6: Semantic concepts

  âœ… Há»c Ä‘Æ°á»£c complex patterns!
```

---

**3. Parameters KhÃ´ng Äá»§**

```
Baseline: 1.4M parameters
  â†’ CÃ³ thá»ƒ há»c ~1.4M patterns
  â†’ Vá»›i 15,777 training images
  â†’ VÃ  224x224x3 high-dimensional data
  â†’ KHÃ”NG Äá»¦ capacity!

MobileNetV2: 2.7M parameters (but pretrained on ImageNet!)
  â†’ ÄÃ£ há»c 1.2M ImageNet images
  â†’ Transfer knowledge to waste classification
  â†’ Äá»§ capacity cho complex task!
```

---

### **B. Continue Training ThÃ­ Nghiá»‡m**

**CÃ¢u há»i:** Náº¿u train thÃªm 20 epochs ná»¯a, accuracy cÃ³ tÄƒng khÃ´ng?

**Káº¿t quáº£:**

```
Epoch 31: Val Acc = 79.41%
Epoch 32: Val Acc = 79.12%  â† Giáº£m!
Epoch 33: Val Acc = 78.90%  â† Giáº£m thÃªm!
...
Epoch 36: Val Acc = 78.90%

EarlyStopping triggered (patience=5)
Training stopped!
```

**Táº¡i sao GIáº¢M?**

```
1. Model Capacity Exhausted:
   â†’ Model Ä‘Ã£ há»c Háº¾T nhá»¯ng gÃ¬ nÃ³ cÃ³ thá»ƒ
   â†’ Architecture quÃ¡ Ä‘Æ¡n giáº£n
   â†’ KhÃ´ng thá»ƒ improve thÃªm

2. Learning Rate QuÃ¡ Tháº¥p:
   â†’ LR = 0.000003 (very small!)
   â†’ Updates quÃ¡ nhá»
   â†’ KhÃ´ng giÃºp gÃ¬

3. Overfitting:
   â†’ Train Acc tÄƒng (81.28% â†’ 81.70%)
   â†’ Val Acc giáº£m (79.59% â†’ 78.90%)
   â†’ Gap tÄƒng (1.69% â†’ 2.80%)
   â†’ Model Ä‘ang MEMORIZE training data!
```

**Káº¿t luáº­n:**
```
âŒ KHÃ”NG pháº£i do thiáº¿u epochs!
âŒ KHÃ”NG pháº£i do learning rate!
âœ… ÄÃ‚Y LÃ€ ARCHITECTURE LIMITATION!

Solution: Cáº§n model DEEPER, WIDER
â†’ Transfer Learning (MobileNetV2)!
```

---

### **C. Visualization: Loss Landscape**

```
Loss (Baseline stuck here)
 â†‘
 â”‚     â•±â•²
 â”‚    â•±  â•²
 â”‚   â•±    â•²________  â† Local minimum (79.5%)
 â”‚  â•±
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs

Loss (MobileNetV2 reaches here)
 â†‘
 â”‚              â•±â•²
 â”‚             â•±  â•²
 â”‚____________â•±    â•²__  â† Global minimum (93.9%)
 â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs
```

**Baseline bá»‹ stuck vÃ¬:**
- âœ— Architecture constraints
- âœ— Limited receptive field
- âœ— Shallow depth
- âœ— Cannot escape local minimum

---

## ğŸ“ Tá»”NG Káº¾T

### **Baseline CNN Characteristics:**

**Strengths (Äiá»ƒm máº¡nh):**
```
âœ… Simple, dá»… hiá»ƒu
âœ… Train nhanh (~30 mins trÃªn GPU)
âœ… Good baseline (79.59%)
âœ… KhÃ´ng overfitting nghiÃªm trá»ng
âœ… Generalize tá»‘t (train-val gap nhá»)
```

**Weaknesses (Äiá»ƒm yáº¿u):**
```
âŒ Accuracy khÃ´ng cao (79.59%)
âŒ Receptive field nhá» (61x61)
âŒ Depth khÃ´ng Ä‘á»§ (8 layers)
âŒ Struggle vá»›i similar classes
âŒ KhÃ´ng há»c Ä‘Æ°á»£c complex patterns
âŒ Model ceiling á»Ÿ ~80%
```

### **Key Takeaways:**

1. **Baseline CNN Ä‘áº¡t 79.59%** - Acceptable cho baseline!
2. **Model Ceiling** - Architecture limitation, khÃ´ng pháº£i training issue
3. **Receptive Field matters** - 61x61 quÃ¡ nhá» Ä‘á»ƒ nhÃ¬n toÃ n bá»™ object
4. **Depth matters** - 8 layers khÃ´ng Ä‘á»§ há»c complex features
5. **Solution** â†’ Transfer Learning (MobileNetV2: 93.90%)

---

## âœ… CHECKPOINT

**Báº¡n cáº§n hiá»ƒu Ä‘Æ°á»£c:**

- [ ] Baseline CNN cÃ³ 4 conv blocks, 8 conv layers
- [ ] Rescaling layer chuyá»ƒn [0,255] â†’ [0,1]
- [ ] GlobalAveragePooling thay cho Flatten
- [ ] Dropout 0.5 Ä‘á»ƒ prevent overfitting
- [ ] Model Ä‘áº¡t 79.59% accuracy
- [ ] Ceiling á»Ÿ ~80% do architecture limitation
- [ ] Receptive field 61x61 quÃ¡ nhá»
- [ ] Continue training KHÃ”NG giÃºp (capacity exhausted)

**Náº¿u OK â†’** Tiáº¿p tá»¥c `04_Transfer_Learning_Chi_Tiet.md` ğŸš€

---

## ğŸ“ BÃ€I Táº¬P Tá»° KIá»‚M TRA

### **CÃ¢u 1:** Táº¡i sao cáº§n Rescaling layer?

<details>
<summary>ÄÃ¡p Ã¡n</summary>

Rescaling chuyá»ƒn pixel [0,255] â†’ [0,1] Ä‘á»ƒ:
- Training á»•n Ä‘á»‹nh hÆ¡n (input nhá»)
- Gradients khÃ´ng explode
- Weights dá»… há»c hÆ¡n
</details>

### **CÃ¢u 2:** Baseline cÃ³ bao nhiÃªu parameters?

<details>
<summary>ÄÃ¡p Ã¡n</summary>

~1.4M parameters
- Conv layers: ~1.2M
- Dense layers: ~200K
- BatchNorm: ~2K
</details>

### **CÃ¢u 3:** Táº¡i sao continue training lÃ m accuracy GIáº¢M?

<details>
<summary>ÄÃ¡p Ã¡n</summary>

VÃ¬:
1. Model capacity exhausted (architecture quÃ¡ Ä‘Æ¡n giáº£n)
2. LR quÃ¡ tháº¥p (0.000003)
3. Overfitting (memorize training data)
â†’ KHÃ”NG pháº£i training issue, lÃ  architecture limitation!
</details>
