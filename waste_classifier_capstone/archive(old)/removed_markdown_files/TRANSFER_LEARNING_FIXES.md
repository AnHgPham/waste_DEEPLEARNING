# Transfer Learning Performance Fixes

## üî¥ V·∫§N ƒê·ªÄ BAN ƒê·∫¶U (ORIGINAL PROBLEMS)

### 1. **Double Preprocessing Bug** ‚ùå
**L·ªói nghi√™m tr·ªçng nh·∫•t!** D·ªØ li·ªáu b·ªã chu·∫©n h√≥a 2 l·∫ßn:

- `preprocessing.py` line 126: `Rescaling(1./255)` chuy·ªÉn ·∫£nh t·ª´ [0, 255] ‚Üí [0, 1]
- `transfer.py` line 44: `mobilenet_v2.preprocess_input()` mong ƒë·ª£i [0, 255] v√† chuy·ªÉn th√†nh [-1, 1]
- **K·∫øt qu·∫£**: MobileNetV2 nh·∫≠n input trong range [0, 1] thay v√¨ [0, 255], d·∫´n ƒë·∫øn normalization sai ho√†n to√†n!

**H·∫≠u qu·∫£**:
- Validation loss c·ª±c cao (9+)
- Accuracy r·∫•t th·∫•p (~27-44%)
- Model kh√¥ng th·ªÉ h·ªçc ƒë∆∞·ª£c pattern ƒë√∫ng

### 2. **BatchNormalization Always Frozen** ‚ùå
`transfer.py` line 47: `base_model(x, training=False)` ƒë∆∞·ª£c hardcode

**H·∫≠u qu·∫£**:
- Ngay c·∫£ khi fine-tuning (phase 2), BatchNorm layers trong base model v·∫´n d√πng statistics t·ª´ ImageNet
- Kh√¥ng adapt ƒë∆∞·ª£c v·ªõi waste classification dataset
- M·∫•t ƒëi l·ª£i √≠ch c·ªßa fine-tuning

### 3. **Suboptimal Hyperparameters** ‚ö†Ô∏è
- Learning rate qu√° cao cho transfer learning
- Augmentation qu√° y·∫øu
- Classification head ƒë∆°n gi·∫£n
- Dropout rate h∆°i cao

---

## ‚úÖ C√ÅC THAY ƒê·ªîI ƒê√É TH·ª∞C HI·ªÜN (FIXES APPLIED)

### 1. **Fixed Double Preprocessing** ‚úì

**`src/data/preprocessing.py`**:
```python
# REMOVED: normalization_layer = layers.Rescaling(1./255)
# REMOVED: train_ds.map(lambda x, y: (normalization_layer(x), y))

# NOW: Keep images in [0, 255] range for MobileNetV2
# MobileNetV2's preprocess_input will handle normalization to [-1, 1]
```

**`src/data/loader.py`**:
```python
# Added 'normalize' parameter (default: True for baseline, False for transfer learning)
def load_dataset(..., normalize=True):
    if normalize:
        # Only normalize for baseline CNN models
        dataset = dataset.map(lambda x, y: (normalization_layer(x), y))
```

**`src/models/transfer.py`**:
```python
# Input now correctly in [0, 255] range
x = keras.applications.mobilenet_v2.preprocess_input(inputs)  # Converts to [-1, 1]
```

### 2. **Fixed BatchNormalization Training Mode** ‚úì

**`src/models/transfer.py`**:
```python
# BEFORE:
x = base_model(x, training=False)  # ‚ùå Always frozen

# AFTER:
x = base_model(x, training=not freeze_base)  # ‚úÖ Adapts during fine-tuning
# Phase 1: training=False (frozen, use ImageNet statistics)
# Phase 2: training=True (fine-tuning, update BatchNorm statistics)
```

### 3. **Improved Hyperparameters** ‚úì

**`src/config.py`**:

**Learning Rates** (gi·∫£m ƒë·ªÉ stable h∆°n):
```python
# BEFORE:
LEARNING_RATE_TRANSFER_PHASE1 = 1e-3   # 0.001 (qu√° cao!)
LEARNING_RATE_TRANSFER_PHASE2 = 1e-4   # 0.0001

# AFTER:
LEARNING_RATE_TRANSFER_PHASE1 = 1e-4   # 0.0001 (stable)
LEARNING_RATE_TRANSFER_PHASE2 = 1e-5   # 0.00001 (very gentle fine-tuning)
```

**Epochs** (tƒÉng ƒë·ªÉ h·ªçc t·ªët h∆°n):
```python
# BEFORE:
EPOCHS_TRANSFER_PHASE1 = 15
EPOCHS_TRANSFER_PHASE2 = 10

# AFTER:
EPOCHS_TRANSFER_PHASE1 = 20  # +5 epochs
EPOCHS_TRANSFER_PHASE2 = 15  # +5 epochs
```

**Classification Head** (tƒÉng capacity):
```python
# BEFORE:
TRANSFER_DENSE_UNITS = 128     # Nh·ªè
TRANSFER_DROPOUT_RATE = 0.5    # H∆°i cao

# AFTER:
TRANSFER_DENSE_UNITS = 256     # TƒÉng g·∫•p ƒë√¥i
TRANSFER_DROPOUT_RATE = 0.3    # Gi·∫£m ƒë·ªÉ model h·ªçc t·ªët h∆°n
```

**Data Augmentation** (m·∫°nh h∆°n):
```python
# BEFORE:
'rotation_factor': 0.1,     # ¬±36 degrees
'zoom_factor': 0.1,
'contrast_factor': 0.1,
'brightness_factor': 0.0,   # Disabled
'width_shift_factor': 0.0,  # Disabled
'height_shift_factor': 0.0, # Disabled

# AFTER:
'rotation_factor': 0.2,      # ¬±72 degrees
'zoom_factor': 0.2,
'contrast_factor': 0.2,
'brightness_factor': 0.1,    # ‚úÖ Enabled
'width_shift_factor': 0.1,   # ‚úÖ Enabled
'height_shift_factor': 0.1,  # ‚úÖ Enabled
```

### 4. **Deeper Classification Head** ‚úì

**`src/models/transfer.py`**:
```python
# BEFORE: 1 dense layer
x = layers.Dense(128, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(num_classes, activation='softmax')(x)

# AFTER: 2 dense layers (more capacity)
x = layers.Dense(256, activation='relu', name="Dense_1")(x)
x = layers.BatchNormalization(name="BatchNorm_1")(x)
x = layers.Dropout(0.3, name="Dropout_1")(x)

x = layers.Dense(128, activation='relu', name="Dense_2")(x)
x = layers.BatchNormalization(name="BatchNorm_2")(x)
x = layers.Dropout(0.3, name="Dropout_2")(x)

outputs = layers.Dense(num_classes, activation='softmax', name="Classifier")(x)
```

### 5. **Enhanced Data Augmentation Pipeline** ‚úì

**`src/data/preprocessing.py`**:
- Th√™m `RandomBrightness` layer
- Th√™m `RandomTranslation` layer
- Augmentation m·∫°nh h∆°n ƒë·ªÉ model generalize t·ªët h∆°n

---

## üöÄ C√ÅCH S·ª¨ D·ª§NG (HOW TO USE)

### X√≥a models c≈© v√† train l·∫°i:
```bash
# Delete old models (they were trained with wrong preprocessing!)
rm outputs/models/mobilenetv2_phase1.keras
rm outputs/models/mobilenetv2_final.keras

# Train again with fixed code
python scripts/04_transfer_learning.py
```

### Ho·∫∑c v·ªõi custom parameters:
```bash
python scripts/04_transfer_learning.py --phase1-epochs 25 --phase2-epochs 20 --unfreeze-layers 50
```

---

## üìä K·∫æT QU·∫¢ D·ª∞ KI·∫æN (EXPECTED RESULTS)

### Tr∆∞·ªõc khi fix:
- ‚ùå Phase 1 Val Accuracy: ~27-30%
- ‚ùå Phase 2 Val Accuracy: ~10-40% (r·∫•t kh√¥ng stable)
- ‚ùå Val Loss: 4-9 (c·ª±c cao)

### Sau khi fix:
- ‚úÖ Phase 1 Val Accuracy: **~75-85%** (feature extraction)
- ‚úÖ Phase 2 Val Accuracy: **~85-92%** (fine-tuning)
- ‚úÖ Val Loss: **<1.0** (normal range)
- ‚úÖ Training stable, kh√¥ng c√≤n spike l·ªõn

---

## üîç T√ìM T·∫ÆT TECHNICAL

### Root Cause:
**Data preprocessing pipeline incompatible v·ªõi MobileNetV2's expected input range**

### Solution:
1. Remove redundant `Rescaling(1./255)` t·ª´ data pipeline
2. Keep images in [0, 255] range
3. Let `mobilenet_v2.preprocess_input()` handle normalization to [-1, 1]
4. Fix BatchNorm training mode
5. Optimize hyperparameters v√† architecture

### Key Lesson:
**Khi d√πng pretrained models, PH·∫¢I ki·ªÉm tra input preprocessing requirements!**
- MobileNetV2: expects [0, 255] ‚Üí normalizes to [-1, 1]
- ResNet/VGG: expects [0, 255] ‚Üí normalizes with mean subtraction
- EfficientNet: expects [0, 255] ‚Üí normalizes to [0, 1]
- Inception: expects [-1, 1]

**NEVER mix preprocessing methods!**

---

## üìù NOTES

- Baseline CNN model v·∫´n c·∫ßn `Rescaling(1./255)` v√¨ n√≥ ƒë∆∞·ª£c train t·ª´ scratch
- Ch·ªâ transfer learning models m·ªõi b·ªè rescaling v√† d√πng model-specific preprocessing
- Lu√¥n ƒë·ªçc documentation c·ªßa pretrained model ƒë·ªÉ hi·ªÉu input requirements!

---

**Fixed by:** AI Assistant  
**Date:** October 30, 2025  
**Impact:** Critical - fixes major bug causing complete model failure

