# Week 2: Transfer Learning with MobileNetV2

## ğŸ“š Má»¥c tiÃªu há»c táº­p

Tuáº§n nÃ y báº¡n sáº½ há»c:
1. **Transfer Learning** - TÃ¡i sá»­ dá»¥ng pretrained models
2. **MobileNetV2 Architecture** - Efficient CNN cho mobile/edge devices
3. **Two-Phase Training** - Feature extraction â†’ Fine-tuning

---

## ğŸ¯ LÃ½ thuyáº¿t Transfer Learning

### 1. Transfer Learning lÃ  gÃ¬?

**Äá»‹nh nghÄ©a:** Sá»­ dá»¥ng kiáº¿n thá»©c (weights) Ä‘Ã£ há»c tá»« task nÃ y cho task khÃ¡c.

**Táº¡i sao hiá»‡u quáº£?**
- ğŸ“ Lower layers há»c **general features** (edges, colors, textures)
- ğŸ¯ Higher layers há»c **specific features** (cho task cá»¥ thá»ƒ)
- ğŸ’ª Pretrained trÃªn ImageNet (1.4M images, 1000 classes) â†’ cÃ³ kiáº¿n thá»©c rá»™ng

**Khi nÃ o dÃ¹ng Transfer Learning?**
- âœ… Dataset nhá» (< 10k images)
- âœ… Task tÆ°Æ¡ng tá»± (cÃ¹ng domain: computer vision)
- âœ… Cáº§n training nhanh
- âœ… Limited computational resources

### 2. MobileNetV2 Architecture

**Äáº·c Ä‘iá»ƒm:**
- ğŸ“± Thiáº¿t káº¿ cho mobile/edge devices
- âš¡ Efficient: 3.5M parameters (vs ResNet50: 25M)
- ğŸ¯ Accuracy cao: ~72% top-1 on ImageNet
- ğŸ”§ Depthwise Separable Convolutions

#### Inverted Residual Block

**Cáº¥u trÃºc cÆ¡ báº£n cá»§a MobileNetV2:**

```
Input
  â†“
[Expansion] 1Ã—1 Conv (expand channels 6x)
  â†“
[Depthwise] 3Ã—3 Depthwise Conv (spatial filtering)
  â†“
[Projection] 1Ã—1 Conv (reduce channels)
  â†“
[Residual Connection] Add input (if stride=1)
```

**Linear Bottleneck:**
- KhÃ´ng dÃ¹ng ReLU á»Ÿ output layer cuá»‘i
- Giá»¯ information flow tá»‘t hÆ¡n

**Depthwise Separable Convolution:**

Thay vÃ¬ standard convolution:
```
Standard Conv: C_in Ã— C_out Ã— K Ã— K parameters
```

TÃ¡ch thÃ nh 2 bÆ°á»›c:
```
1. Depthwise: C_in Ã— K Ã— K (filter tá»«ng channel riÃªng)
2. Pointwise: C_in Ã— C_out Ã— 1 Ã— 1 (káº¿t há»£p channels)
â†’ Giáº£m ~8-9x parameters
```

### 3. Two-Phase Training Strategy

#### Phase 1: Feature Extraction (Frozen Base)

```python
base_model.trainable = False  # Freeze all base layers
```

**Má»¥c Ä‘Ã­ch:**
- Train chá»‰ classification head (top layers)
- Sá»­ dá»¥ng features Ä‘Ã£ há»c tá»« ImageNet
- Nhanh, á»•n Ä‘á»‹nh

**Hyperparameters:**
- Learning rate: 0.001 (cao hÆ¡n)
- Epochs: 15
- Freeze: 100% base model

**Expected:** 80-85% accuracy

#### Phase 2: Fine-Tuning (Unfrozen Top Layers)

```python
# Unfreeze top 30 layers
for layer in base_model.layers[-30:]:
    layer.trainable = True
```

**Má»¥c Ä‘Ã­ch:**
- Adapt features cho waste classification
- Há»c features specific cho domain
- Improve accuracy

**Hyperparameters:**
- Learning rate: 0.0001 (tháº¥p hÆ¡n 10x)
- Epochs: 10
- Unfreeze: Top 30 layers (~20% cá»§a base)

**Expected:** 85-92% accuracy

**âš ï¸ Quan trá»ng:**
- **Pháº£i train Phase 1 trÆ°á»›c** Phase 2
- **Lower learning rate** trong Phase 2 (trÃ¡nh phÃ¡ há»ng pretrained weights)
- **Unfreeze tá»« tá»«** (khÃ´ng unfreeze táº¥t cáº£ ngay)

---

## ğŸ“‚ Cáº¥u trÃºc

```
Week2_Transfer_Learning/
â”œâ”€â”€ README.md                    # File nÃ y
â”œâ”€â”€ assignments/                 # Notebooks
â”‚   â”œâ”€â”€ W2_Feature_Extraction.ipynb
â”‚   â””â”€â”€ W2_Fine_Tuning.ipynb
â”œâ”€â”€ transfer_learning.py         # Script (full pipeline)
â””â”€â”€ utils/
    â””â”€â”€ model_utils.py           # build_transfer_model, unfreeze_layers
```

---

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### Python Script (Recommended)

```bash
# Cháº¡y full pipeline (Phase 1 + Phase 2)
python Week2_Transfer_Learning/transfer_learning.py

# Custom epochs
python Week2_Transfer_Learning/transfer_learning.py --phase1-epochs 20 --phase2-epochs 15

# Hoáº·c dÃ¹ng main.py
python main.py --week 2
```

### Jupyter Notebooks (Step-by-step)

```bash
cd Week2_Transfer_Learning/assignments
jupyter notebook W2_Feature_Extraction.ipynb
```

---

## ğŸ“Š Kiáº¿n trÃºc Model

```
Input (224Ã—224Ã—3)
    â†“
[Preprocessing] MobileNetV2 specific normalization
    â†“
[Base Model: MobileNetV2]
    - 154 layers
    - Pretrained on ImageNet
    - Output: 1280 features
    â†“
[Classification Head]
    GlobalAveragePooling2D
    â†“
    Dense(128, ReLU)
    â†“
    BatchNormalization
    â†“
    Dropout(0.5)
    â†“
    Dense(10, Softmax)
```

**Total parameters:** ~3.5M  
**Trainable (Phase 1):** ~170K (only head)  
**Trainable (Phase 2):** ~1.2M (head + top 30 layers)

---

## ğŸ“ˆ Káº¿t quáº£ mong Ä‘á»£i

| Metric | Phase 1 | Phase 2 |
|--------|---------|---------|
| Val Accuracy | 80-85% | 85-92% |
| Training Time (GPU) | 10-15 min | 8-10 min |
| Parameters Trained | 170K | 1.2M |

**Improvement vs Baseline:**
- ğŸ“ˆ +10-15% accuracy
- âš¡ 3x fewer parameters
- ğŸš€ Faster convergence

---

## ğŸ”¬ So sÃ¡nh Training Strategies

### Strategy 1: Feature Extraction Only
```python
base_model.trainable = False
```
- âœ… Nhanh nháº¥t
- âœ… á»”n Ä‘á»‹nh
- âš ï¸ Accuracy trung bÃ¬nh

### Strategy 2: Fine-Tuning All Layers
```python
base_model.trainable = True  # Táº¥t cáº£
```
- âš ï¸ Dá»… overfit
- âš ï¸ Cáº§n dataset lá»›n
- âš ï¸ CÃ³ thá»ƒ phÃ¡ há»ng pretrained weights

### Strategy 3: Two-Phase (RECOMMENDED)
```python
# Phase 1: Frozen
# Phase 2: Unfreeze top layers
```
- âœ… Best accuracy
- âœ… Stable training
- âœ… Balance giá»¯a speed vÃ  performance

---

## ğŸ’¡ Tips vÃ  Best Practices

### 1. Learning Rate

**Rule of thumb:**
```
Fine-tuning LR = Transfer LR / 10
```

**Táº¡i sao?**
- Pretrained weights Ä‘Ã£ tá»‘t
- Chá»‰ cáº§n Ä‘iá»u chá»‰nh nháº¹
- LR cao â†’ phÃ¡ há»ng features Ä‘Ã£ há»c

### 2. Sá»‘ layers unfreeze

**Guidelines:**
- Small dataset (< 5k): Unfreeze 10-20 layers
- Medium dataset (5-10k): Unfreeze 20-40 layers
- Large dataset (> 10k): Unfreeze 40-80 layers

**Vá»›i waste dataset (~20k images):**
â†’ Unfreeze 30 layers lÃ  há»£p lÃ½

### 3. Data Augmentation

**Váº«n cáº§n!** DÃ¹ Ä‘Ã£ dÃ¹ng transfer learning:
- âœ… Horizontal flip
- âœ… Rotation (Â±10%)
- âœ… Zoom (Â±10%)
- âœ… Contrast adjustment

### 4. Batch Size

**MobileNetV2 vá»›i 224Ã—224:**
- GPU 8GB: batch_size = 32-64
- GPU 16GB: batch_size = 64-128

---

## ğŸ”— TÃ i liá»‡u tham kháº£o

1. **Transfer Learning:**
   - "A Survey on Transfer Learning" - Pan & Yang (2010)
   - CS231n: Transfer Learning lecture

2. **MobileNetV2:**
   - "MobileNetV2: Inverted Residuals and Linear Bottlenecks" (2018)
   - Original paper: https://arxiv.org/abs/1801.04381

3. **Fine-tuning:**
   - "How transferable are features in deep neural networks?" (2014)
   - Keras Applications Documentation

---

## ğŸ“ Kiáº¿n thá»©c há»c Ä‘Æ°á»£c

Sau Week 2, báº¡n sáº½ hiá»ƒu:

âœ… Transfer learning vÃ  khi nÃ o dÃ¹ng  
âœ… MobileNetV2 architecture vÃ  depthwise convolutions  
âœ… Two-phase training strategy  
âœ… Fine-tuning best practices  
âœ… Cáº£i thiá»‡n 10-15% accuracy so vá»›i baseline  

---

**Previous:** [Week 1 - Baseline CNN](../Week1_Data_and_Baseline/README.md)  
**Next:** [Week 3 - Real-time Detection](../Week3_Realtime_Detection/README.md)

