# Week 4: Model Optimization and Deployment

## ğŸ“š Má»¥c tiÃªu há»c táº­p

Tuáº§n nÃ y báº¡n sáº½ há»c:
1. **Model Optimization** - Giáº£m kÃ­ch thÆ°á»›c, tÄƒng tá»‘c Ä‘á»™
2. **Quantization** - INT8, FP16
3. **TensorFlow Lite** - Deployment cho mobile/edge
4. **Performance Trade-offs** - Size vs Accuracy

---

## ğŸ¯ LÃ½ thuyáº¿t Model Optimization

### 1. Táº¡i sao cáº§n Optimization?

**Váº¥n Ä‘á» vá»›i models gá»‘c:**
- ğŸ“¦ KÃ­ch thÆ°á»›c lá»›n (MobileNetV2: ~15 MB)
- ğŸŒ Inference cháº­m trÃªn edge devices
- ğŸ’° Memory footprint cao
- ğŸ”‹ TiÃªu thá»¥ nÄƒng lÆ°á»£ng nhiá»u

**Target devices:**
- ğŸ“± Smartphones (Android/iOS)
- ğŸ¥§ Raspberry Pi
- ğŸ¤– Edge AI devices (Jetson Nano)
- ğŸŒ Web browsers (TensorFlow.js)

### 2. TensorFlow Lite

**Äá»‹nh nghÄ©a:** Framework Ä‘á»ƒ run TensorFlow models trÃªn mobile/embedded devices.

**Äáº·c Ä‘iá»ƒm:**
- âœ… Lightweight runtime
- âœ… Low latency
- âœ… Nhá» gá»n (< 1MB)
- âœ… Hardware acceleration support

**Conversion process:**
```
TensorFlow/Keras Model (.keras)
        â†“
TFLite Converter
        â†“
TFLite Model (.tflite)
```

### 3. Quantization

#### a) Floating Point (FP32)

**Standard format:**
- 32 bits per weight
- Range: Â±3.4 Ã— 10Â³â¸
- Precision: 7 decimal digits

**Example:**
```
Weight = 0.12345678 â†’ Stored as FP32
```

#### b) INT8 Quantization

**CÃ´ng thá»©c chuyá»ƒn Ä‘á»•i:**
```
Quantized = round(FP32_value / scale) + zero_point

Where:
- scale = (max - min) / 255
- zero_point: offset value
```

**Example:**
```
FP32 range: [-1.0, 1.0]
â†’ scale = 2.0 / 255 â‰ˆ 0.0078
â†’ INT8 range: [-128, 127]

FP32 value: 0.5
â†’ INT8: round(0.5 / 0.0078) = 64
```

**Benefits:**
- ğŸ“¦ 4Ã— smaller (32 bits â†’ 8 bits)
- âš¡ 2-4Ã— faster inference
- ğŸ’¾ Less memory bandwidth
- ğŸ”‹ Lower power consumption

**Trade-off:**
- âš ï¸ Accuracy drop: 1-3% (acceptable)

#### c) Post-Training Quantization

**3 types:**

**1. Dynamic Range Quantization**
```python
converter.optimizations = [tf.lite.Optimize.DEFAULT]
```
- âœ… Dá»… nháº¥t, khÃ´ng cáº§n data
- âš ï¸ Activation váº«n FP32

**2. Full Integer Quantization**
```python
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = gen_func
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
```
- âœ… Weights + Activations Ä‘á»u INT8
- âœ… Maximum speed-up
- âš ï¸ Cáº§n representative dataset

**3. Float16 Quantization**
```python
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]
```
- âœ… 2Ã— smaller
- âœ… Minimal accuracy loss
- âš ï¸ Chá»‰ trÃªn GPU há»— trá»£ FP16

#### Representative Dataset

**Má»¥c Ä‘Ã­ch:** Calibrate quantization ranges

**YÃªu cáº§u:**
- ~100-500 samples tá»« training set
- Äáº¡i diá»‡n cho distribution of data
- Cháº¡y qua model Ä‘á»ƒ capture activation ranges

**Code:**
```python
def representative_data_gen():
    for input_value in dataset.take(100):
        yield [input_value]

converter.representative_dataset = representative_data_gen
```

---

## ğŸ“Š Optimization Pipeline

### Step 1: Baseline (Keras Model)

```
mobilenetv2_final.keras
- Format: FP32
- Size: ~15 MB
- Accuracy: 90%
```

### Step 2: Convert to TFLite FP32

```python
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
```

**Result:**
```
mobilenetv2_fp32.tflite
- Format: FP32
- Size: ~14 MB (slight reduction)
- Accuracy: ~90% (same)
- Speed: 1.2Ã— faster
```

### Step 3: Quantize to INT8

```python
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = gen_func
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
tflite_quant_model = converter.convert()
```

**Result:**
```
mobilenetv2_int8.tflite
- Format: INT8
- Size: ~900 KB (16Ã— reduction!)
- Accuracy: ~87-88% (2-3% drop)
- Speed: 3-4Ã— faster
```

---

## ğŸ“‚ Cáº¥u trÃºc

```
Week4_Deployment/
â”œâ”€â”€ README.md                    # File nÃ y
â”œâ”€â”€ assignments/                 # Notebook
â”‚   â””â”€â”€ W4_Model_Optimization.ipynb
â”œâ”€â”€ model_optimization.py        # Main script
â””â”€â”€ utils/
    â””â”€â”€ optimization_utils.py    # Convert, quantize, evaluate
```

---

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### Python Script

```bash
# Optimize model
python Week4_Deployment/model_optimization.py

# Chá»n model cá»¥ thá»ƒ
python Week4_Deployment/model_optimization.py --model mobilenetv2

# Hoáº·c main.py
python main.py --optimize --model mobilenetv2
```

### Output

Script sáº½ táº¡o:
```
outputs/models/
â”œâ”€â”€ mobilenetv2_fp32.tflite     # FP32 TFLite
â””â”€â”€ mobilenetv2_int8.tflite     # INT8 quantized
```

VÃ  in comparison:
```
Optimization Summary:
  Model Type          Size (MB)    Accuracy    Size Reduction
  ----------------------------------------------------------------
  Original Keras         14.50      0.9012      baseline
  TFLite FP32            13.80      0.9010      4.8%
  TFLite INT8             0.90      0.8788      93.8%
```

---

## âš–ï¸ Trade-offs Analysis

### Size vs Accuracy

| Model | Size | Accuracy | Use Case |
|-------|------|----------|----------|
| Keras FP32 | 15 MB | 90% | Server, High-end GPU |
| TFLite FP32 | 14 MB | 90% | Desktop, Laptop |
| TFLite INT8 | 900 KB | 87-88% | Mobile, Raspberry Pi |

**Recommendation:**
- ğŸ–¥ï¸ **Server/Cloud:** Keras FP32
- ğŸ’» **Desktop app:** TFLite FP32
- ğŸ“± **Mobile/Edge:** TFLite INT8

### Speed vs Accuracy

**Inference Time (1 image):**

| Device | Keras FP32 | TFLite FP32 | TFLite INT8 |
|--------|------------|-------------|-------------|
| Desktop GPU | 5 ms | 4 ms | 2 ms |
| Laptop CPU | 50 ms | 40 ms | 15 ms |
| Raspberry Pi 4 | 500 ms | 400 ms | 100 ms |
| Smartphone | 200 ms | 150 ms | 50 ms |

**â†’ INT8 lÃ  3-4Ã— nhanh hÆ¡n trÃªn edge devices!**

---

## ğŸ”§ Deployment Strategies

### 1. Raspberry Pi

```python
import tflite_runtime.interpreter as tflite

# Load model
interpreter = tflite.Interpreter(model_path="mobilenetv2_int8.tflite")
interpreter.allocate_tensors()

# Inference
input_details = interpreter.get_input_details()[0]
output_details = interpreter.get_output_details()[0]

interpreter.set_tensor(input_details['index'], input_data)
interpreter.invoke()
output = interpreter.get_tensor(output_details['index'])
```

**Install:**
```bash
pip install tflite-runtime
```

### 2. Android (Kotlin/Java)

```kotlin
// Load model
val interpreter = Interpreter(loadModelFile())

// Prepare input
val inputArray = arrayOf(imageData)
val outputArray = arrayOf(FloatArray(10))

// Inference
interpreter.run(inputArray, outputArray)
val predictions = outputArray[0]
```

### 3. iOS (Swift)

```swift
// Load model
let model = try VNCoreMLModel(for: WasteClassifier().model)

// Create request
let request = VNCoreMLRequest(model: model) { request, error in
    guard let results = request.results as? [VNClassificationObservation]
    else { return }
    // Process results
}
```

### 4. Web (TensorFlow.js)

```javascript
// Load model
const model = await tf.loadGraphModel('model.json');

// Preprocess
const tensor = tf.browser.fromPixels(imageElement)
    .resizeBilinear([224, 224])
    .expandDims(0)
    .div(255.0);

// Inference
const predictions = await model.predict(tensor).data();
```

---

## ğŸ“ˆ Benchmarking

### Metrics to measure

**1. Model Size**
```python
size_mb = os.path.getsize(model_path) / (1024 * 1024)
```

**2. Inference Time**
```python
import time
start = time.time()
predictions = model.predict(image)
latency = time.time() - start
```

**3. Accuracy**
```python
test_loss, test_acc = model.evaluate(test_dataset)
```

**4. Memory Usage**
```python
import psutil
process = psutil.Process()
memory_mb = process.memory_info().rss / (1024 * 1024)
```

---

## ğŸ’¡ Best Practices

### 1. Always benchmark

**Before deployment:**
```
âœ“ Test on target device
âœ“ Measure actual latency
âœ“ Check memory consumption
âœ“ Verify accuracy on test set
```

### 2. Calibration data

**Representative dataset:**
- DÃ¹ng 100-500 samples tá»« training set
- Shuffle Ä‘á»ƒ diverse
- KhÃ´ng dÃ¹ng augmentation

### 3. Quantization-aware training

**For best INT8 accuracy:**
```python
import tensorflow_model_optimization as tfmot

# Quantization-aware training
quantize_model = tfmot.quantization.keras.quantize_model
q_aware_model = quantize_model(model)

# Train
q_aware_model.fit(...)

# Convert to TFLite INT8
converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)
```

â†’ Accuracy drop < 1%!

### 4. Fallback strategies

```python
# Try INT8 first
if accuracy_int8 > threshold:
    use_int8_model()
else:
    # Fallback to FP16
    use_fp16_model()
```

---

## ğŸ“ Kiáº¿n thá»©c há»c Ä‘Æ°á»£c

Sau Week 4, báº¡n sáº½ hiá»ƒu:

âœ… Model optimization techniques  
âœ… Quantization (INT8, FP16)  
âœ… TensorFlow Lite conversion  
âœ… Size-Accuracy-Speed trade-offs  
âœ… Deployment strategies cho mobile/edge  
âœ… Benchmarking vÃ  profiling  

---

## ğŸ”— TÃ i liá»‡u tham kháº£o

1. **TensorFlow Lite:**
   - Official TFLite Documentation
   - "TensorFlow Lite Guide" - Google

2. **Quantization:**
   - "Quantization and Training of Neural Networks" (2018)
   - "Integer Quantization for Deep Learning Inference" (2020)

3. **Deployment:**
   - "On-Device Machine Learning" - Google I/O
   - TFLite Performance Best Practices

---

**Previous:** [Week 3 - Real-time Detection](../Week3_Realtime_Detection/README.md)  
**Home:** [Project Root](../README.md)

